{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzmCcDvBFqd_"
      },
      "source": [
        "[Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)\n",
        "\n",
        "[Implementation guide](https://amaarora.github.io/posts/2020-08-02-densenets.html)\n",
        "\n",
        "[Easy to understand blog post](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgCgCk9Sc_Sb",
        "outputId": "024e6a51-7940-4457-f103-c356b966efb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Lzj1LMIGCUbE"
      },
      "outputs": [],
      "source": [
        "FOLDERNAME = \"cs231n/assignments/assignment2/\"\n",
        "assert FOLDERNAME is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2xiI5MSbCa8d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive/{}\".format(FOLDERNAME))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARuuQ1EyCdaL",
        "outputId": "ad1ed6c7-0a44-4794-91b5-72ec9622f6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/cs231n/assignments/assignment2/cs231n/datasets\n",
            "/content/drive/My Drive/cs231n/assignments/assignment2\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
        "!bash get_datasets.sh\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9IfZ9-JCgio",
        "outputId": "26633e03-b54b-4e88-bf8c-56f7f1eebaf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "USE_GPU = True\n",
        "dtype = torch.float32\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "print_every = 100\n",
        "\n",
        "print(\"using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQT5TUiBCiwD",
        "outputId": "7507fd39-b1c9-40c2-e8f2-e8792857d07d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "NUM_TRAIN = 49000\n",
        "\n",
        "transform = T.Compose([T.ToTensor(),\n",
        "                       T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "            ])\n",
        "\n",
        "cifar10_train = dset.CIFAR10(\"./cs231n/datasets\", train=True, download=True, transform=transform)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=64, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar10_val = dset.CIFAR10(\"./cs231n/datasets\", train=True, download=True, transform=transform)\n",
        "loader_val = DataLoader(cifar10_val, batch_size=64, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar10_test = dset.CIFAR10(\"./cs231n/datasets\", train = False, download = True, transform=transform)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GtwybqMRClw7"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "bClWelcJHzNO"
      },
      "outputs": [],
      "source": [
        "class TransitionLayer(nn.Sequential):\n",
        "  \"Transition layers between dense blocks\"\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    \"\"\"\n",
        "    Reduces the depth of the activation maps\n",
        "\n",
        "    Args:\n",
        "      in_channels (int)\n",
        "      out_channels (int)\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.add_module('norm', nn.BatchNorm2d(in_channels))\n",
        "    self.add_module('relu', nn.ReLU(inplace=True))\n",
        "    #downsampling the depth of the activation map\n",
        "    self.add_module('conv', nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False))\n",
        "    self.add_module('pool', nn.AvgPool2d(kernel_size=2))\n",
        "\n",
        "class DenseLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  Layer used in each DenseBlock in DenseNet\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, growth_rate, bottleneck_size, drop_rate):\n",
        "\n",
        "    \"\"\"\n",
        "    For the computational efficiency reasons, first we reduce the input depth\n",
        "    to growth_rate*bottleneck_size by appliying convolution with kernel_size=1\n",
        "    Then, we perform the convolution with kernel_size=3\n",
        "\n",
        "    Args:\n",
        "      in_channels (int)\n",
        "      growth_rate (int): how much the feature maps grows with each layer\n",
        "      bottleneck_size (int): a multiplier that determines the depth of the intermediate feature maps after kernel_size=1 convolution\n",
        "      drop_rate (int)\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.norm1 = nn.BatchNorm2d(in_channels)\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "    self.conv1 = nn.Conv2d(in_channels, growth_rate*bottleneck_size, kernel_size=1, bias=False)\n",
        "\n",
        "    self.norm2 = nn.BatchNorm2d(growth_rate*bottleneck_size)\n",
        "    self.relu2 = nn.ReLU(inplace=True)\n",
        "    self.conv2 = nn.Conv2d(growth_rate*bottleneck_size, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    self.drop_rate = float(drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "          x (Tensor): Input data of dim (M, N, in_channels, H, W).\n",
        "                      M corresponds to the number of the elements in the list of tensors\n",
        "\n",
        "    Returns:\n",
        "          output (Tensor): Output data of dim (N, growth_rate, H, W)\n",
        "    \"\"\"\n",
        "    x = [x] if torch.is_tensor(x) else x\n",
        "    x =  self.conv1(self.relu1(self.norm1(torch.cat(x, 1))))\n",
        "    output = self.conv2(self.relu2(self.norm2(x)))\n",
        "\n",
        "    if self.drop_rate > 0:\n",
        "      output = F.dropout(output, p=self.drop_rate, training=self.training)\n",
        "    return output\n",
        "\n",
        "class DenseBlock(nn.ModuleDict):\n",
        "  \"\"\"\n",
        "  Blocks of DenseLayers used in DenseNet\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_layers, in_channels, growth_rate, bottleneck_size, drop_rate):\n",
        "    \"\"\"\n",
        "    DenseBlock connects num_layers of DenseLayer(s) with shared hyperparameters.\n",
        "    The output of each DenseLayer(s) serves as the input for each of all sebsequent DenseLayer(s)\n",
        "\n",
        "    Args:\n",
        "      num_layers (int)\n",
        "      in_channels (int)\n",
        "      growth_rate (int): how much the feature maps grows with each layer\n",
        "      bottleneck_size (int): a multiplier that determines the depth of the intermediate feature maps after kernel_size=1 convolution\n",
        "      drop_rate (float)\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    for i in range(num_layers):\n",
        "      layer = DenseLayer(in_channels+i*growth_rate, growth_rate, bottleneck_size, drop_rate)\n",
        "      self.add_module(f\"danselayer{i+1}\", layer)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "          x (Tensor): Input data of dim (N, in_channels, H, W).\n",
        "\n",
        "    Returns:\n",
        "          output (Tensor): Output data of dim (N, growth_rate, H, W)\n",
        "    \"\"\"\n",
        "    input = [x]\n",
        "\n",
        "    for name, layer in self.items():\n",
        "      output = layer(input)\n",
        "      input.append(output)\n",
        "\n",
        "    return torch.cat(input, 1)\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "  \"\"\"\n",
        "  Architecture:\n",
        "    1. NORM->ReLU->CONV\n",
        "    2. DenseBlock->[TransitionLayer->DenseBlock] * N\n",
        "    3. NORM->ReLU-POOL-LINEAR\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, block_config=(12,12,12), in_channels=32, growth_rate=16, bottleneck_size=4, drop_rate=0, num_classes=10):\n",
        "    \"\"\"\n",
        "    The first sequence NORM->ReLU->CONV produces in_channels activation maps that is fed into the sequence of blocks,\n",
        "    each with the number of layers according to the block_config, spaced by the transition layer.\n",
        "    At the end of the last dense block, a global average pooling is performed before the softmax classifier.\n",
        "\n",
        "    Args:\n",
        "        block_config (tuple): number of layers for each consequtive denseblock\n",
        "        in_channels (int)\n",
        "        growth_rate (int): how much the feature maps grows with each layer\n",
        "        bottleneck_size (int): a multiplier that determines the depth of the intermediate feature maps after kernel_size=1 convolution\n",
        "        drop_rate (float)\n",
        "        num_classes (int)\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.features = nn.Sequential(OrderedDict([\n",
        "        ('norm0', nn.BatchNorm2d(3)),\n",
        "        ('relu0', nn.ReLU(inplace=True)),\n",
        "        ('conv0', nn.Conv2d(3, in_channels, kernel_size=7, padding=3, bias=False))\n",
        "    ]))\n",
        "\n",
        "    num_features = in_channels\n",
        "\n",
        "    for i, num_layers in enumerate(block_config):\n",
        "      block = DenseBlock(num_layers, num_features, growth_rate, bottleneck_size, drop_rate)\n",
        "      self.features.add_module(f'denseblock{i+1}', block)\n",
        "      num_features += num_layers*growth_rate\n",
        "\n",
        "      if i != len(block_config) - 1:\n",
        "        trans = TransitionLayer(num_features, num_features//2)\n",
        "        self.features.add_module(f'transition{i+1}', trans)\n",
        "        num_features = num_features//2\n",
        "\n",
        "    self.features.add_module(f'norm{i+2}', nn.BatchNorm2d(num_features))\n",
        "    self.features.add_module(f'relu{i+2}', nn.ReLU(inplace=True))\n",
        "\n",
        "    self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x(Tensor): input data of dim (N, 3, H, W)\n",
        "\n",
        "    Returns:\n",
        "      Out(Tensor): output data of dim (N, 10)\n",
        "    \"\"\"\n",
        "    out = F.adaptive_avg_pool2d(self.features(x), (1,1))\n",
        "    out = self.classifier(torch.flatten(out, start_dim=1))\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "5x_qLJTo7Cv_"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.02\n",
        "\n",
        "model = DenseNet()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "wEqMEh3p7rRK"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model):\n",
        "  if loader.dataset.train:\n",
        "    print(\"Checking accuracy on validation set\")\n",
        "  else:\n",
        "    print(\"Checking accuracy on test set\")\n",
        "  num_correct, num_samples = 0, 0\n",
        "  model.to(device=device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device=device, dtype=dtype)\n",
        "      y = y.to(device=device, dtype=torch.long)\n",
        "      scores = model(x)\n",
        "      _, preds = scores.max(1)\n",
        "      num_correct += (y==preds).sum()\n",
        "      num_samples += preds.size(0)\n",
        "    acc = float(num_correct/num_samples)\n",
        "    print((\"Got %d / %d correct (%.2f)\") % (num_correct, num_samples, acc*100))\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "hLVJVhkB7SEA"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, epochs=1):\n",
        "  model = model.to(device=device)\n",
        "  for e in range(epochs):\n",
        "    for t, (x,y) in enumerate(loader_train):\n",
        "      model.train()\n",
        "      x = x.to(device=device, dtype=dtype)\n",
        "      y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "      scores = model(x)\n",
        "      loss = F.cross_entropy(scores, y)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if t%print_every == 0:\n",
        "        print(\"Iteration %d, loss = %.4f\" % (t, loss.item()))\n",
        "        check_accuracy(loader_val, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i18SSPRY9W8r",
        "outputId": "7f8074d9-812b-4a91-a382-1ddb19d0bdf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss = 2.5389\n",
            "Checking accuracy on validation set\n",
            "Got 87 / 1000 correct (8.70)\n",
            "Iteration 100, loss = 2.0260\n",
            "Checking accuracy on validation set\n",
            "Got 219 / 1000 correct (21.90)\n",
            "Iteration 200, loss = 1.8031\n",
            "Checking accuracy on validation set\n",
            "Got 283 / 1000 correct (28.30)\n",
            "Iteration 300, loss = 1.7538\n",
            "Checking accuracy on validation set\n",
            "Got 281 / 1000 correct (28.10)\n",
            "Iteration 400, loss = 1.6534\n",
            "Checking accuracy on validation set\n",
            "Got 355 / 1000 correct (35.50)\n",
            "Iteration 500, loss = 1.4788\n",
            "Checking accuracy on validation set\n",
            "Got 407 / 1000 correct (40.70)\n",
            "Iteration 600, loss = 1.5120\n",
            "Checking accuracy on validation set\n",
            "Got 402 / 1000 correct (40.20)\n",
            "Iteration 700, loss = 1.5767\n",
            "Checking accuracy on validation set\n",
            "Got 407 / 1000 correct (40.70)\n",
            "Iteration 0, loss = 1.7096\n",
            "Checking accuracy on validation set\n",
            "Got 291 / 1000 correct (29.10)\n",
            "Iteration 100, loss = 1.2818\n",
            "Checking accuracy on validation set\n",
            "Got 455 / 1000 correct (45.50)\n",
            "Iteration 200, loss = 1.2541\n",
            "Checking accuracy on validation set\n",
            "Got 519 / 1000 correct (51.90)\n",
            "Iteration 300, loss = 1.0922\n",
            "Checking accuracy on validation set\n",
            "Got 443 / 1000 correct (44.30)\n",
            "Iteration 400, loss = 1.0628\n",
            "Checking accuracy on validation set\n",
            "Got 563 / 1000 correct (56.30)\n",
            "Iteration 500, loss = 0.9579\n",
            "Checking accuracy on validation set\n",
            "Got 622 / 1000 correct (62.20)\n",
            "Iteration 600, loss = 1.1263\n",
            "Checking accuracy on validation set\n",
            "Got 557 / 1000 correct (55.70)\n",
            "Iteration 700, loss = 1.0083\n",
            "Checking accuracy on validation set\n",
            "Got 606 / 1000 correct (60.60)\n",
            "Iteration 0, loss = 0.9723\n",
            "Checking accuracy on validation set\n",
            "Got 563 / 1000 correct (56.30)\n",
            "Iteration 100, loss = 0.9038\n",
            "Checking accuracy on validation set\n",
            "Got 607 / 1000 correct (60.70)\n",
            "Iteration 200, loss = 0.8239\n",
            "Checking accuracy on validation set\n",
            "Got 604 / 1000 correct (60.40)\n",
            "Iteration 300, loss = 0.5879\n",
            "Checking accuracy on validation set\n",
            "Got 645 / 1000 correct (64.50)\n",
            "Iteration 400, loss = 0.8953\n",
            "Checking accuracy on validation set\n",
            "Got 590 / 1000 correct (59.00)\n",
            "Iteration 500, loss = 0.7035\n",
            "Checking accuracy on validation set\n",
            "Got 637 / 1000 correct (63.70)\n",
            "Iteration 600, loss = 0.8390\n",
            "Checking accuracy on validation set\n",
            "Got 654 / 1000 correct (65.40)\n",
            "Iteration 700, loss = 0.5199\n",
            "Checking accuracy on validation set\n",
            "Got 699 / 1000 correct (69.90)\n",
            "Iteration 0, loss = 1.0639\n",
            "Checking accuracy on validation set\n",
            "Got 697 / 1000 correct (69.70)\n",
            "Iteration 100, loss = 0.5218\n",
            "Checking accuracy on validation set\n",
            "Got 705 / 1000 correct (70.50)\n",
            "Iteration 200, loss = 0.5066\n",
            "Checking accuracy on validation set\n",
            "Got 737 / 1000 correct (73.70)\n",
            "Iteration 300, loss = 0.7111\n",
            "Checking accuracy on validation set\n",
            "Got 718 / 1000 correct (71.80)\n",
            "Iteration 400, loss = 0.6841\n",
            "Checking accuracy on validation set\n",
            "Got 766 / 1000 correct (76.60)\n",
            "Iteration 500, loss = 0.6161\n",
            "Checking accuracy on validation set\n",
            "Got 745 / 1000 correct (74.50)\n",
            "Iteration 600, loss = 0.6752\n",
            "Checking accuracy on validation set\n",
            "Got 739 / 1000 correct (73.90)\n",
            "Iteration 700, loss = 0.8361\n",
            "Checking accuracy on validation set\n",
            "Got 709 / 1000 correct (70.90)\n",
            "Iteration 0, loss = 0.8224\n",
            "Checking accuracy on validation set\n",
            "Got 726 / 1000 correct (72.60)\n",
            "Iteration 100, loss = 0.8042\n",
            "Checking accuracy on validation set\n",
            "Got 755 / 1000 correct (75.50)\n",
            "Iteration 200, loss = 0.9351\n",
            "Checking accuracy on validation set\n",
            "Got 694 / 1000 correct (69.40)\n",
            "Iteration 300, loss = 0.4438\n",
            "Checking accuracy on validation set\n",
            "Got 786 / 1000 correct (78.60)\n",
            "Iteration 400, loss = 0.5903\n",
            "Checking accuracy on validation set\n",
            "Got 781 / 1000 correct (78.10)\n",
            "Iteration 500, loss = 0.5358\n",
            "Checking accuracy on validation set\n",
            "Got 776 / 1000 correct (77.60)\n",
            "Iteration 600, loss = 0.4126\n",
            "Checking accuracy on validation set\n",
            "Got 777 / 1000 correct (77.70)\n",
            "Iteration 700, loss = 0.8075\n",
            "Checking accuracy on validation set\n",
            "Got 793 / 1000 correct (79.30)\n",
            "Iteration 0, loss = 0.4936\n",
            "Checking accuracy on validation set\n",
            "Got 804 / 1000 correct (80.40)\n",
            "Iteration 100, loss = 0.4796\n",
            "Checking accuracy on validation set\n",
            "Got 805 / 1000 correct (80.50)\n",
            "Iteration 200, loss = 0.4691\n",
            "Checking accuracy on validation set\n",
            "Got 788 / 1000 correct (78.80)\n",
            "Iteration 300, loss = 0.5342\n",
            "Checking accuracy on validation set\n",
            "Got 798 / 1000 correct (79.80)\n",
            "Iteration 400, loss = 0.5362\n",
            "Checking accuracy on validation set\n",
            "Got 815 / 1000 correct (81.50)\n",
            "Iteration 500, loss = 0.7609\n",
            "Checking accuracy on validation set\n",
            "Got 795 / 1000 correct (79.50)\n",
            "Iteration 600, loss = 0.9307\n",
            "Checking accuracy on validation set\n",
            "Got 813 / 1000 correct (81.30)\n",
            "Iteration 700, loss = 0.4819\n",
            "Checking accuracy on validation set\n",
            "Got 811 / 1000 correct (81.10)\n",
            "Iteration 0, loss = 0.3262\n",
            "Checking accuracy on validation set\n",
            "Got 830 / 1000 correct (83.00)\n",
            "Iteration 100, loss = 0.5170\n",
            "Checking accuracy on validation set\n",
            "Got 805 / 1000 correct (80.50)\n",
            "Iteration 200, loss = 0.5929\n",
            "Checking accuracy on validation set\n",
            "Got 826 / 1000 correct (82.60)\n",
            "Iteration 300, loss = 0.7423\n",
            "Checking accuracy on validation set\n",
            "Got 829 / 1000 correct (82.90)\n",
            "Iteration 400, loss = 0.2935\n",
            "Checking accuracy on validation set\n",
            "Got 813 / 1000 correct (81.30)\n",
            "Iteration 500, loss = 0.3992\n",
            "Checking accuracy on validation set\n",
            "Got 814 / 1000 correct (81.40)\n",
            "Iteration 600, loss = 0.3777\n",
            "Checking accuracy on validation set\n",
            "Got 778 / 1000 correct (77.80)\n",
            "Iteration 700, loss = 0.4782\n",
            "Checking accuracy on validation set\n",
            "Got 827 / 1000 correct (82.70)\n",
            "Iteration 0, loss = 0.3777\n",
            "Checking accuracy on validation set\n",
            "Got 814 / 1000 correct (81.40)\n",
            "Iteration 100, loss = 0.2795\n",
            "Checking accuracy on validation set\n",
            "Got 824 / 1000 correct (82.40)\n",
            "Iteration 200, loss = 0.4603\n",
            "Checking accuracy on validation set\n",
            "Got 814 / 1000 correct (81.40)\n",
            "Iteration 300, loss = 0.4023\n",
            "Checking accuracy on validation set\n",
            "Got 791 / 1000 correct (79.10)\n",
            "Iteration 400, loss = 0.4214\n",
            "Checking accuracy on validation set\n",
            "Got 828 / 1000 correct (82.80)\n",
            "Iteration 500, loss = 0.4567\n",
            "Checking accuracy on validation set\n",
            "Got 801 / 1000 correct (80.10)\n",
            "Iteration 600, loss = 0.4829\n",
            "Checking accuracy on validation set\n",
            "Got 831 / 1000 correct (83.10)\n",
            "Iteration 700, loss = 0.4231\n",
            "Checking accuracy on validation set\n",
            "Got 853 / 1000 correct (85.30)\n",
            "Iteration 0, loss = 0.2736\n",
            "Checking accuracy on validation set\n",
            "Got 821 / 1000 correct (82.10)\n",
            "Iteration 100, loss = 0.3230\n",
            "Checking accuracy on validation set\n",
            "Got 849 / 1000 correct (84.90)\n",
            "Iteration 200, loss = 0.2004\n",
            "Checking accuracy on validation set\n",
            "Got 846 / 1000 correct (84.60)\n",
            "Iteration 300, loss = 0.4404\n",
            "Checking accuracy on validation set\n",
            "Got 807 / 1000 correct (80.70)\n",
            "Iteration 400, loss = 0.5985\n",
            "Checking accuracy on validation set\n",
            "Got 816 / 1000 correct (81.60)\n",
            "Iteration 500, loss = 0.5487\n",
            "Checking accuracy on validation set\n",
            "Got 843 / 1000 correct (84.30)\n",
            "Iteration 600, loss = 0.3140\n",
            "Checking accuracy on validation set\n",
            "Got 850 / 1000 correct (85.00)\n",
            "Iteration 700, loss = 0.3322\n",
            "Checking accuracy on validation set\n",
            "Got 845 / 1000 correct (84.50)\n",
            "Iteration 0, loss = 0.2741\n",
            "Checking accuracy on validation set\n",
            "Got 836 / 1000 correct (83.60)\n",
            "Iteration 100, loss = 0.3969\n",
            "Checking accuracy on validation set\n",
            "Got 837 / 1000 correct (83.70)\n",
            "Iteration 200, loss = 0.4334\n",
            "Checking accuracy on validation set\n",
            "Got 837 / 1000 correct (83.70)\n",
            "Iteration 300, loss = 0.3332\n",
            "Checking accuracy on validation set\n",
            "Got 819 / 1000 correct (81.90)\n",
            "Iteration 400, loss = 0.3281\n",
            "Checking accuracy on validation set\n",
            "Got 857 / 1000 correct (85.70)\n",
            "Iteration 500, loss = 0.2753\n",
            "Checking accuracy on validation set\n",
            "Got 827 / 1000 correct (82.70)\n",
            "Iteration 600, loss = 0.4510\n",
            "Checking accuracy on validation set\n",
            "Got 826 / 1000 correct (82.60)\n",
            "Iteration 700, loss = 0.3092\n",
            "Checking accuracy on validation set\n",
            "Got 859 / 1000 correct (85.90)\n"
          ]
        }
      ],
      "source": [
        "train(model, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8dHL1I0Ba01",
        "outputId": "3bd77d85-fb0c-4630-d6ff-c653e8a159fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking accuracy on test set\n",
            "Got 8220 / 10000 correct (82.20)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8219999670982361"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "best_model = model\n",
        "check_accuracy(loader_test, best_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wehqkmbOpKs0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}