{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87816c67",
   "metadata": {},
   "source": [
    "# Auxiliary functions to load CIFAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3630044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import platform\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, \"data_batch_%d\" % (b,))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, \"test_batch\"))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict[\"data\"]\n",
    "        Y = datadict[\"labels\"]\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "    \n",
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == \"2\":\n",
    "        return pickle.load(f)\n",
    "    elif version[0] == \"3\":\n",
    "        return pickle.load(f, encoding=\"latin1\")\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c574d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ef58a",
   "metadata": {},
   "source": [
    "# Load CIFAR dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00086a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d453a",
   "metadata": {},
   "source": [
    "# SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb43a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Loss funtion: implementation with loops\n",
    "\n",
    "    There are C classes, we operate on minibatches of N examples, and inputs have dimension D\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    num_train = X.shape[0]\n",
    "    num_dims = X.shape[1]\n",
    "    num_classes = W.shape[1]\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(num_train):\n",
    "        fs = np.zeros(num_classes)\n",
    "        for j in range(num_classes):\n",
    "            fs[j] = X[i] @ W[:, j]\n",
    "        fs = fs - fs.max() #for numerical stability of exponentiation/division\n",
    "        probs = np.exp(fs)/np.sum(np.exp(fs))\n",
    "        loss += -np.log(probs[y[i]])\n",
    "        \n",
    "        probs[y[i]] -= 1     \n",
    "        dW += np.outer(X[i, :], probs)\n",
    "\n",
    "    loss /= num_train\n",
    "    loss += reg*np.sum(W**2)\n",
    "\n",
    "    dW /= num_train\n",
    "    dW += reg*2*W \n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4da73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.310664\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40477853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical in this dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evaluate f(x + h)\n",
    "        x[ix] = oldval - h  # increment by h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
    "            abs(grad_numerical) + abs(grad_analytic)\n",
    "        )\n",
    "        print(\n",
    "            \"numerical: %f analytic: %f, relative error: %e\"\n",
    "            % (grad_numerical, grad_analytic, rel_error)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc89a162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.340114 analytic: 0.340114, relative error: 1.022008e-07\n",
      "numerical: -2.031211 analytic: -2.031211, relative error: 9.128577e-09\n",
      "numerical: 0.226404 analytic: 0.226404, relative error: 1.837265e-08\n",
      "numerical: 0.106643 analytic: 0.106643, relative error: 1.283587e-07\n",
      "numerical: -2.948276 analytic: -2.948276, relative error: 1.391475e-08\n",
      "numerical: 0.672194 analytic: 0.672194, relative error: 1.340894e-08\n",
      "numerical: -1.495760 analytic: -1.495760, relative error: 1.362676e-08\n",
      "numerical: -2.486766 analytic: -2.486766, relative error: 5.193148e-09\n",
      "numerical: 2.080274 analytic: 2.080273, relative error: 4.061204e-08\n",
      "numerical: 0.676472 analytic: 0.676472, relative error: 4.345756e-08\n",
      "numerical: -1.133299 analytic: -1.133300, relative error: 9.095495e-08\n",
      "numerical: -0.655991 analytic: -0.655991, relative error: 1.122232e-08\n",
      "numerical: -0.658423 analytic: -0.658423, relative error: 8.635434e-09\n",
      "numerical: -0.309786 analytic: -0.309786, relative error: 1.224031e-07\n",
      "numerical: 1.704655 analytic: 1.704655, relative error: 2.091524e-08\n",
      "numerical: 1.054677 analytic: 1.054677, relative error: 2.722342e-09\n",
      "numerical: 1.789881 analytic: 1.789881, relative error: 1.146245e-09\n",
      "numerical: 0.579211 analytic: 0.579211, relative error: 1.220731e-07\n",
      "numerical: -0.356969 analytic: -0.356969, relative error: 2.879155e-08\n",
      "numerical: -1.153334 analytic: -1.153334, relative error: 1.504020e-08\n"
     ]
    }
   ],
   "source": [
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# Another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ed8ba",
   "metadata": {},
   "source": [
    "### Vectorized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3272ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    num_train = X.shape[0]\n",
    "    num_dims = X.shape[1]\n",
    "    num_classes = W.shape[1]\n",
    "    loss = 0\n",
    "    fs = np.zeros((num_train, num_classes))     \n",
    "    \n",
    "    fs = X @ W\n",
    "    fs = fs - fs.max(axis = 1, keepdims = True) #exponents that are numerically stable\n",
    "    probs = np.exp(fs)/np.sum(np.exp(fs), axis = 1, keepdims = True)\n",
    "    loss = -np.sum(np.log(probs[range(num_train), y]))\n",
    "\n",
    "    probs[range(num_train), y] -= 1\n",
    "    dW = np.dot(X.T, probs)\n",
    "\n",
    "    loss = loss/num_train + reg*np.sum(W**2)\n",
    "    dW /= num_train\n",
    "    dW += reg*2*W \n",
    "    \n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8b20c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.310664e+00 computed in 0.073085s\n",
      "vectorized loss: 2.310664e+00 computed in 0.006444s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# Frobenius norm to compare the two versions of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8d4a1",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c9ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        learning_rate=1e-3,\n",
    "        reg=1e-5,\n",
    "        num_iters=100,\n",
    "        batch_size=200,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train this classifier using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "          means that X[i] has label 0 <= c < C for C classes.\n",
    "        - learning_rate: (float) learning rate for optimization.\n",
    "        - reg: (float) regularization strength.\n",
    "        - num_iters: (integer) number of steps to take when optimizing.\n",
    "        - batch_size: (integer) number of training examples to use at each step.\n",
    "        - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "        Outputs:\n",
    "        A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = (np.max(y) + 1)\n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            inds = np.random.choice(num_train, size = batch_size)\n",
    "            X_batch = X[inds]\n",
    "            y_batch = y[inds]\n",
    "\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            self.W -= learning_rate*grad\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict labels for data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted labels for the data in X. \n",
    "            y_pred is a 1-dimensional array of length N, and each element is an integer giving the predicted class.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        scores = X@self.W\n",
    "        y_pred = np.argmax(scores, axis = 1)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative.\n",
    "        Subclasses will override this.\n",
    "\n",
    "        Inputs:\n",
    "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N data points; each point has dimension D.\n",
    "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "        - reg: (float) regularization strength.\n",
    "\n",
    "        Returns: A tuple containing:\n",
    "        - loss as a single float\n",
    "        - gradient with respect to self.W; an array of the same shape as W\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class Softmax(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8bfb3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 37.032505\n",
      "iteration 100 / 1000: loss 33.372618\n",
      "iteration 200 / 1000: loss 31.967352\n",
      "iteration 300 / 1000: loss 30.360112\n",
      "iteration 400 / 1000: loss 28.766938\n",
      "iteration 500 / 1000: loss 27.595878\n",
      "iteration 600 / 1000: loss 26.611958\n",
      "iteration 700 / 1000: loss 25.577488\n",
      "iteration 800 / 1000: loss 24.639453\n",
      "iteration 900 / 1000: loss 23.680559\n",
      "iteration 0 / 1000: loss 106.323937\n",
      "iteration 100 / 1000: loss 91.759710\n",
      "iteration 200 / 1000: loss 80.632472\n",
      "iteration 300 / 1000: loss 70.811538\n",
      "iteration 400 / 1000: loss 62.116345\n",
      "iteration 500 / 1000: loss 54.683418\n",
      "iteration 600 / 1000: loss 48.137582\n",
      "iteration 700 / 1000: loss 42.613545\n",
      "iteration 800 / 1000: loss 37.473201\n",
      "iteration 900 / 1000: loss 33.025555\n",
      "iteration 0 / 1000: loss 172.950937\n",
      "iteration 100 / 1000: loss 137.149036\n",
      "iteration 200 / 1000: loss 109.942489\n",
      "iteration 300 / 1000: loss 88.054601\n",
      "iteration 400 / 1000: loss 70.901584\n",
      "iteration 500 / 1000: loss 57.043413\n",
      "iteration 600 / 1000: loss 46.183285\n",
      "iteration 700 / 1000: loss 37.351228\n",
      "iteration 800 / 1000: loss 30.310122\n",
      "iteration 900 / 1000: loss 24.740544\n",
      "iteration 0 / 1000: loss 244.801532\n",
      "iteration 100 / 1000: loss 178.043601\n",
      "iteration 200 / 1000: loss 130.799606\n",
      "iteration 300 / 1000: loss 96.104071\n",
      "iteration 400 / 1000: loss 70.943348\n",
      "iteration 500 / 1000: loss 52.530576\n",
      "iteration 600 / 1000: loss 38.795433\n",
      "iteration 700 / 1000: loss 28.960888\n",
      "iteration 800 / 1000: loss 21.751548\n",
      "iteration 900 / 1000: loss 16.340698\n",
      "iteration 0 / 1000: loss 313.962848\n",
      "iteration 100 / 1000: loss 209.212807\n",
      "iteration 200 / 1000: loss 140.613383\n",
      "iteration 300 / 1000: loss 94.489015\n",
      "iteration 400 / 1000: loss 63.835892\n",
      "iteration 500 / 1000: loss 43.311361\n",
      "iteration 600 / 1000: loss 29.610491\n",
      "iteration 700 / 1000: loss 20.410174\n",
      "iteration 800 / 1000: loss 14.294643\n",
      "iteration 900 / 1000: loss 10.261854\n",
      "iteration 0 / 1000: loss 36.359093\n",
      "iteration 100 / 1000: loss 29.735074\n",
      "iteration 200 / 1000: loss 26.056908\n",
      "iteration 300 / 1000: loss 22.776390\n",
      "iteration 400 / 1000: loss 20.107548\n",
      "iteration 500 / 1000: loss 18.019211\n",
      "iteration 600 / 1000: loss 15.826361\n",
      "iteration 700 / 1000: loss 14.192810\n",
      "iteration 800 / 1000: loss 12.557187\n",
      "iteration 900 / 1000: loss 11.152915\n",
      "iteration 0 / 1000: loss 105.819546\n",
      "iteration 100 / 1000: loss 67.746888\n",
      "iteration 200 / 1000: loss 44.530135\n",
      "iteration 300 / 1000: loss 29.683144\n",
      "iteration 400 / 1000: loss 20.047040\n",
      "iteration 500 / 1000: loss 13.706023\n",
      "iteration 600 / 1000: loss 9.572202\n",
      "iteration 700 / 1000: loss 6.914959\n",
      "iteration 800 / 1000: loss 5.200141\n",
      "iteration 900 / 1000: loss 3.972689\n",
      "iteration 0 / 1000: loss 173.984188\n",
      "iteration 100 / 1000: loss 84.541353\n",
      "iteration 200 / 1000: loss 41.944466\n",
      "iteration 300 / 1000: loss 21.408369\n",
      "iteration 400 / 1000: loss 11.452779\n",
      "iteration 500 / 1000: loss 6.465606\n",
      "iteration 600 / 1000: loss 4.180234\n",
      "iteration 700 / 1000: loss 3.002305\n",
      "iteration 800 / 1000: loss 2.568554\n",
      "iteration 900 / 1000: loss 2.208698\n",
      "iteration 0 / 1000: loss 246.787251\n",
      "iteration 100 / 1000: loss 89.849754\n",
      "iteration 200 / 1000: loss 33.627211\n",
      "iteration 300 / 1000: loss 13.459964\n",
      "iteration 400 / 1000: loss 6.191248\n",
      "iteration 500 / 1000: loss 3.476382\n",
      "iteration 600 / 1000: loss 2.477493\n",
      "iteration 700 / 1000: loss 2.196196\n",
      "iteration 800 / 1000: loss 2.029796\n",
      "iteration 900 / 1000: loss 1.990767\n",
      "iteration 0 / 1000: loss 313.215031\n",
      "iteration 100 / 1000: loss 85.723506\n",
      "iteration 200 / 1000: loss 24.583251\n",
      "iteration 300 / 1000: loss 8.039232\n",
      "iteration 400 / 1000: loss 3.673296\n",
      "iteration 500 / 1000: loss 2.536150\n",
      "iteration 600 / 1000: loss 2.109031\n",
      "iteration 700 / 1000: loss 1.999508\n",
      "iteration 800 / 1000: loss 1.960042\n",
      "iteration 900 / 1000: loss 2.008155\n",
      "iteration 0 / 1000: loss 36.581818\n",
      "iteration 100 / 1000: loss 27.376133\n",
      "iteration 200 / 1000: loss 22.002418\n",
      "iteration 300 / 1000: loss 17.859321\n",
      "iteration 400 / 1000: loss 14.748852\n",
      "iteration 500 / 1000: loss 12.041849\n",
      "iteration 600 / 1000: loss 10.021148\n",
      "iteration 700 / 1000: loss 8.308757\n",
      "iteration 800 / 1000: loss 7.026462\n",
      "iteration 900 / 1000: loss 5.820212\n",
      "iteration 0 / 1000: loss 106.933839\n",
      "iteration 100 / 1000: loss 51.272502\n",
      "iteration 200 / 1000: loss 25.768429\n",
      "iteration 300 / 1000: loss 13.470283\n",
      "iteration 400 / 1000: loss 7.602098\n",
      "iteration 500 / 1000: loss 4.593157\n",
      "iteration 600 / 1000: loss 3.257020\n",
      "iteration 700 / 1000: loss 2.582203\n",
      "iteration 800 / 1000: loss 2.152763\n",
      "iteration 900 / 1000: loss 1.958676\n",
      "iteration 0 / 1000: loss 175.057951\n",
      "iteration 100 / 1000: loss 52.380826\n",
      "iteration 200 / 1000: loss 16.816252\n",
      "iteration 300 / 1000: loss 6.308948\n",
      "iteration 400 / 1000: loss 3.282552\n",
      "iteration 500 / 1000: loss 2.232130\n",
      "iteration 600 / 1000: loss 2.097628\n",
      "iteration 700 / 1000: loss 1.922859\n",
      "iteration 800 / 1000: loss 1.906131\n",
      "iteration 900 / 1000: loss 1.987608\n",
      "iteration 0 / 1000: loss 240.626522\n",
      "iteration 100 / 1000: loss 44.243888\n",
      "iteration 200 / 1000: loss 9.489456\n",
      "iteration 300 / 1000: loss 3.301494\n",
      "iteration 400 / 1000: loss 2.239971\n",
      "iteration 500 / 1000: loss 2.071863\n",
      "iteration 600 / 1000: loss 1.990605\n",
      "iteration 700 / 1000: loss 1.980560\n",
      "iteration 800 / 1000: loss 2.013552\n",
      "iteration 900 / 1000: loss 1.884850\n",
      "iteration 0 / 1000: loss 316.775475\n",
      "iteration 100 / 1000: loss 35.987848\n",
      "iteration 200 / 1000: loss 5.647544\n",
      "iteration 300 / 1000: loss 2.446684\n",
      "iteration 400 / 1000: loss 2.055401\n",
      "iteration 500 / 1000: loss 1.983712\n",
      "iteration 600 / 1000: loss 1.954701\n",
      "iteration 700 / 1000: loss 2.004465\n",
      "iteration 800 / 1000: loss 2.002847\n",
      "iteration 900 / 1000: loss 1.993706\n",
      "iteration 0 / 1000: loss 35.907511\n",
      "iteration 100 / 1000: loss 24.785274\n",
      "iteration 200 / 1000: loss 18.502934\n",
      "iteration 300 / 1000: loss 13.894083\n",
      "iteration 400 / 1000: loss 10.673179\n",
      "iteration 500 / 1000: loss 8.207016\n",
      "iteration 600 / 1000: loss 6.535827\n",
      "iteration 700 / 1000: loss 5.273531\n",
      "iteration 800 / 1000: loss 4.264566\n",
      "iteration 900 / 1000: loss 3.712254\n",
      "iteration 0 / 1000: loss 106.890008\n",
      "iteration 100 / 1000: loss 38.795073\n",
      "iteration 200 / 1000: loss 15.126867\n",
      "iteration 300 / 1000: loss 6.742305\n",
      "iteration 400 / 1000: loss 3.688613\n",
      "iteration 500 / 1000: loss 2.482806\n",
      "iteration 600 / 1000: loss 2.086354\n",
      "iteration 700 / 1000: loss 1.949982\n",
      "iteration 800 / 1000: loss 2.028226\n",
      "iteration 900 / 1000: loss 1.944344\n",
      "iteration 0 / 1000: loss 177.943359\n",
      "iteration 100 / 1000: loss 32.720262\n",
      "iteration 200 / 1000: loss 7.409774\n",
      "iteration 300 / 1000: loss 2.903884\n",
      "iteration 400 / 1000: loss 2.138239\n",
      "iteration 500 / 1000: loss 2.074053\n",
      "iteration 600 / 1000: loss 1.957731\n",
      "iteration 700 / 1000: loss 2.013477\n",
      "iteration 800 / 1000: loss 1.956595\n",
      "iteration 900 / 1000: loss 1.931630\n",
      "iteration 0 / 1000: loss 245.000105\n",
      "iteration 100 / 1000: loss 23.087128\n",
      "iteration 200 / 1000: loss 3.833467\n",
      "iteration 300 / 1000: loss 2.121704\n",
      "iteration 400 / 1000: loss 2.004180\n",
      "iteration 500 / 1000: loss 2.043232\n",
      "iteration 600 / 1000: loss 1.993550\n",
      "iteration 700 / 1000: loss 1.964222\n",
      "iteration 800 / 1000: loss 1.970155\n",
      "iteration 900 / 1000: loss 2.035914\n",
      "iteration 0 / 1000: loss 314.299660\n",
      "iteration 100 / 1000: loss 15.406321\n",
      "iteration 200 / 1000: loss 2.592521\n",
      "iteration 300 / 1000: loss 2.011773\n",
      "iteration 400 / 1000: loss 2.082988\n",
      "iteration 500 / 1000: loss 2.048744\n",
      "iteration 600 / 1000: loss 2.000800\n",
      "iteration 700 / 1000: loss 2.041293\n",
      "iteration 800 / 1000: loss 1.997792\n",
      "iteration 900 / 1000: loss 1.962444\n",
      "iteration 0 / 1000: loss 37.205143\n",
      "iteration 100 / 1000: loss 23.092937\n",
      "iteration 200 / 1000: loss 15.667148\n",
      "iteration 300 / 1000: loss 11.070306\n",
      "iteration 400 / 1000: loss 8.023814\n",
      "iteration 500 / 1000: loss 5.882693\n",
      "iteration 600 / 1000: loss 4.517190\n",
      "iteration 700 / 1000: loss 3.664304\n",
      "iteration 800 / 1000: loss 3.108292\n",
      "iteration 900 / 1000: loss 2.647288\n",
      "iteration 0 / 1000: loss 104.469399\n",
      "iteration 100 / 1000: loss 28.541751\n",
      "iteration 200 / 1000: loss 9.092330\n",
      "iteration 300 / 1000: loss 3.812173\n",
      "iteration 400 / 1000: loss 2.469165\n",
      "iteration 500 / 1000: loss 2.017447\n",
      "iteration 600 / 1000: loss 1.911297\n",
      "iteration 700 / 1000: loss 1.957686\n",
      "iteration 800 / 1000: loss 1.940276\n",
      "iteration 900 / 1000: loss 1.890139\n",
      "iteration 0 / 1000: loss 177.691897\n",
      "iteration 100 / 1000: loss 20.651052\n",
      "iteration 200 / 1000: loss 3.951970\n",
      "iteration 300 / 1000: loss 2.184729\n",
      "iteration 400 / 1000: loss 1.925320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 / 1000: loss 1.929240\n",
      "iteration 600 / 1000: loss 1.975377\n",
      "iteration 700 / 1000: loss 1.933624\n",
      "iteration 800 / 1000: loss 2.047878\n",
      "iteration 900 / 1000: loss 1.980599\n",
      "iteration 0 / 1000: loss 243.627364\n",
      "iteration 100 / 1000: loss 12.288538\n",
      "iteration 200 / 1000: loss 2.432172\n",
      "iteration 300 / 1000: loss 2.009451\n",
      "iteration 400 / 1000: loss 2.018159\n",
      "iteration 500 / 1000: loss 2.069090\n",
      "iteration 600 / 1000: loss 2.047536\n",
      "iteration 700 / 1000: loss 1.989479\n",
      "iteration 800 / 1000: loss 1.966102\n",
      "iteration 900 / 1000: loss 2.008686\n",
      "iteration 0 / 1000: loss 311.427726\n",
      "iteration 100 / 1000: loss 7.333757\n",
      "iteration 200 / 1000: loss 2.069608\n",
      "iteration 300 / 1000: loss 1.995267\n",
      "iteration 400 / 1000: loss 2.059871\n",
      "iteration 500 / 1000: loss 1.964225\n",
      "iteration 600 / 1000: loss 2.012957\n",
      "iteration 700 / 1000: loss 2.031487\n",
      "iteration 800 / 1000: loss 1.937864\n",
      "iteration 900 / 1000: loss 2.005086\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.243571 val accuracy: 0.228000\n",
      "lr 1.000000e-07 reg 3.250000e+03 train accuracy: 0.266082 val accuracy: 0.247000\n",
      "lr 1.000000e-07 reg 5.500000e+03 train accuracy: 0.296612 val accuracy: 0.299000\n",
      "lr 1.000000e-07 reg 7.750000e+03 train accuracy: 0.316061 val accuracy: 0.305000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.334265 val accuracy: 0.338000\n",
      "lr 3.250000e-07 reg 1.000000e+03 train accuracy: 0.323694 val accuracy: 0.317000\n",
      "lr 3.250000e-07 reg 3.250000e+03 train accuracy: 0.371612 val accuracy: 0.381000\n",
      "lr 3.250000e-07 reg 5.500000e+03 train accuracy: 0.368551 val accuracy: 0.375000\n",
      "lr 3.250000e-07 reg 7.750000e+03 train accuracy: 0.361122 val accuracy: 0.382000\n",
      "lr 3.250000e-07 reg 1.000000e+04 train accuracy: 0.360694 val accuracy: 0.372000\n",
      "lr 5.500000e-07 reg 1.000000e+03 train accuracy: 0.370347 val accuracy: 0.388000\n",
      "lr 5.500000e-07 reg 3.250000e+03 train accuracy: 0.381653 val accuracy: 0.395000\n",
      "lr 5.500000e-07 reg 5.500000e+03 train accuracy: 0.369082 val accuracy: 0.393000\n",
      "lr 5.500000e-07 reg 7.750000e+03 train accuracy: 0.352306 val accuracy: 0.354000\n",
      "lr 5.500000e-07 reg 1.000000e+04 train accuracy: 0.353367 val accuracy: 0.360000\n",
      "lr 7.750000e-07 reg 1.000000e+03 train accuracy: 0.390000 val accuracy: 0.380000\n",
      "lr 7.750000e-07 reg 3.250000e+03 train accuracy: 0.377755 val accuracy: 0.395000\n",
      "lr 7.750000e-07 reg 5.500000e+03 train accuracy: 0.367306 val accuracy: 0.377000\n",
      "lr 7.750000e-07 reg 7.750000e+03 train accuracy: 0.359571 val accuracy: 0.368000\n",
      "lr 7.750000e-07 reg 1.000000e+04 train accuracy: 0.354000 val accuracy: 0.364000\n",
      "lr 1.000000e-06 reg 1.000000e+03 train accuracy: 0.394571 val accuracy: 0.405000\n",
      "lr 1.000000e-06 reg 3.250000e+03 train accuracy: 0.378041 val accuracy: 0.392000\n",
      "lr 1.000000e-06 reg 5.500000e+03 train accuracy: 0.362857 val accuracy: 0.371000\n",
      "lr 1.000000e-06 reg 7.750000e+03 train accuracy: 0.354469 val accuracy: 0.363000\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.341469 val accuracy: 0.355000\n",
      "best validation accuracy achieved during cross-validation: 0.405000\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "learning_rates = np.linspace(1e-7, 1e-6, 5)\n",
    "regularization_strengths = np.linspace(1e3, 1e4, 5)\n",
    "\n",
    "for learning_rate, regularization_strength in itertools.product(learning_rates, regularization_strengths):\n",
    "\n",
    "    smax = Softmax()\n",
    "    loss = smax.train(X_train, y_train, learning_rate, regularization_strength,\n",
    "                    num_iters=1000, verbose=True)\n",
    "  \n",
    "    y_train_pred = smax.predict(X_train)\n",
    "    training_accuracy = np.mean(y_train == y_train_pred)\n",
    "\n",
    "    y_val_pred = smax.predict(X_val)\n",
    "    validation_accuracy = np.mean(y_val == y_val_pred)\n",
    "\n",
    "    results[(learning_rate, regularization_strength)] = (training_accuracy, validation_accuracy)\n",
    "    if validation_accuracy > best_val:\n",
    "        best_val = validation_accuracy\n",
    "        best_softmax = smax\n",
    "    \n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36930dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.382000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda0981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
