{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from builtins import range\n",
    "from builtins import object\n",
    "from __future__ import print_function, division\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()\n",
    "from past.builtins import xrange\n",
    "from six.moves import cPickle as pickle\n",
    "from random import randrange\n",
    "from time import time\n",
    "import os\n",
    "import platform\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # Set default size of plots.\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "# http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Useful Functions for Forward and Backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"Computes the forward pass for an affine (fully connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    \n",
    "    x_row = x.reshape(x.shape[0], -1) #shape (N, D)\n",
    "    \n",
    "    out = x_row@w + b #shape (N, M)\n",
    "    cache = (x, w, b)\n",
    "    \n",
    "    return out, cache\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    \"\"\"Computes the backward pass for an affine (fully connected) layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "\n",
    "    x_row = x.reshape(x.shape[0], -1) #shape (N, D)\n",
    "    dw = x_row.T@dout\n",
    "    db = np.sum(dout, axis=0)\n",
    "    dx = (dout@w.T).reshape(x.shape[0], *x.shape[1:])\n",
    "\n",
    "    return dx, dw, db\n",
    "\n",
    "def relu_forward(x):\n",
    "    \"\"\"Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "\n",
    "    out = np.maximum(0, x)\n",
    "    cache = x\n",
    "    \n",
    "    return out, cache\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    \"\"\"Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    dx = dout*(x > 0)\n",
    "\n",
    "    return dx\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "    \"\"\"Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and 0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    loss, dx = None, None\n",
    "\n",
    "    num_train = x.shape[0]\n",
    "    num_classes = x.shape[1]\n",
    "    fs = x     \n",
    "    fs = fs - fs.max(axis = 1, keepdims = True) #exponents that are numerically stable\n",
    "    probs = np.exp(fs)/np.sum(np.exp(fs), axis = 1, keepdims = True)\n",
    "    loss = -np.sum(np.log(probs[range(num_train), y]))\n",
    "    loss = loss/num_train\n",
    "\n",
    "    probs[range(num_train), y] -= 1\n",
    "    dx = probs/num_train\n",
    "\n",
    "    return loss, dx\n",
    "\n",
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"Forward pass for batch normalization.\n",
    "\n",
    "    During training the sample mean and (uncorrected) sample variance are computed from minibatch statistics and used to normalize the incoming data.\n",
    "    During training we also keep an exponentially decaying running mean of the mean and variance of each feature, and these averages are used to normalize\n",
    "    data at test-time.\n",
    "\n",
    "    At each timestep we update the running averages for mean and variance using an exponential decay based on the momentum parameter:\n",
    "\n",
    "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    mode = bn_param[\"mode\"]\n",
    "    eps = bn_param.get(\"eps\", 1e-5)\n",
    "    momentum = bn_param.get(\"momentum\", 0.9)\n",
    "\n",
    "    N, D = x.shape\n",
    "    running_mean = bn_param.get(\"running_mean\", np.zeros(D, dtype=x.dtype))\n",
    "    running_var = bn_param.get(\"running_var\", np.zeros(D, dtype=x.dtype))\n",
    "\n",
    "    out, cache = None, None\n",
    "    if mode == \"train\":\n",
    "\n",
    "        mu = x.mean(axis=0)        \n",
    "        var = x.var(axis=0)       \n",
    "        std = np.sqrt(var + eps)   \n",
    "        x_hat = (x - mu) / std     # standartized x\n",
    "        out = gamma * x_hat + beta # scaled and shifted x_hat\n",
    "\n",
    "        shape = bn_param.get('shape', (N, D))              # reshape used in backprop\n",
    "        axis = bn_param.get('axis', 0)                     # axis to sum used in backprop\n",
    "        cache = x, mu, var, std, gamma, x_hat, shape, axis # save for backprop\n",
    "\n",
    "        if axis == 0:                                                    \n",
    "            running_mean = momentum * running_mean + (1 - momentum) * mu # update overall mean\n",
    "            running_var = momentum * running_var + (1 - momentum) * var  # update overall variance\n",
    "\n",
    "    elif mode == \"test\":\n",
    "\n",
    "        x_hat = (x - running_mean) / np.sqrt(running_var + eps)\n",
    "        out = gamma * x_hat + beta\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "\n",
    "    # Store the updated running means back into bn_param\n",
    "    bn_param[\"running_mean\"] = running_mean\n",
    "    bn_param[\"running_var\"] = running_var\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "    \"\"\"Backward pass for batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, D)\n",
    "    - cache: Variable of intermediates from batchnorm_forward.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    \n",
    "    # original paper (https://arxiv.org/abs/1502.03167)\n",
    "    # https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "    \n",
    "    x, mu, var, std, gamma, x_hat, shape, axis = cache\n",
    "\n",
    "    dbeta = dout.reshape(shape, order='F').sum(axis)            # derivative w.r.t. beta\n",
    "    dgamma = (dout * x_hat).reshape(shape, order='F').sum(axis) # derivative w.r.t. gamma\n",
    "\n",
    "    dx_hat = dout * gamma                                       # derivative w.r.t. x_hat\n",
    "    dstd = -np.sum(dx_hat * (x-mu), axis=0) / (std**2)          # derivative w.r.t. std\n",
    "    dvar = 0.5 * dstd / std                                     # derivative w.r.t. var\n",
    "    dx1 = dx_hat / std + 2 * (x-mu) * dvar / len(dout)          # partial derivative w.r.t. dx\n",
    "    dmu = -np.sum(dx1, axis=0)                                  # derivative w.r.t. mu\n",
    "    dx2 = dmu / len(dout)                                       # partial derivative w.r.t. dx\n",
    "    dx = dx1 + dx2                                              # full derivative w.r.t. x\n",
    "\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def batchnorm_backward_alt(dout, cache):\n",
    "    \"\"\"Alternative backward pass for batch normalization.\n",
    "\n",
    "    Inputs / outputs: Same as batchnorm_backward\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "    _, _, _, std, gamma, x_hat, shape, axis = cache # expand cache\n",
    "    S = lambda x: x.sum(axis=0)                     # helper function\n",
    "    \n",
    "    dbeta = dout.reshape(shape, order='F').sum(axis)            # derivative w.r.t. beta\n",
    "    dgamma = (dout * x_hat).reshape(shape, order='F').sum(axis) # derivative w.r.t. gamma\n",
    "    \n",
    "    dx = dout * gamma / (len(dout) * std)          # temporarily initialize scale value\n",
    "    dx = len(dout)*dx  - S(dx*x_hat)*x_hat - S(dx) # derivative w.r.t. unnormalized x\n",
    "\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def layernorm_forward(x, gamma, beta, ln_param):\n",
    "    \"\"\"Forward pass for layer normalization.\n",
    "\n",
    "    During both training and test-time, the incoming data is normalized per data-point,\n",
    "    before being scaled by gamma and beta parameters identical to that of batch normalization.\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - ln_param: Dictionary with the following keys:\n",
    "        - eps: Constant for numeric stability\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "    eps = ln_param.get(\"eps\", 1e-5)\n",
    "    \n",
    "    bn_param = {\"mode\": \"train\", \"axis\": 1, **ln_param} # same as batchnorm in train mode + over which axis to sum for grad\n",
    "    [gamma, beta] = np.atleast_2d(gamma, beta)          # assure 2D to perform transpose\n",
    "\n",
    "    out, cache = batchnorm_forward(x.T, gamma.T, beta.T, bn_param) # same as batchnorm\n",
    "    out = out.T                                                    # transpose back\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def layernorm_backward(dout, cache):\n",
    "    \"\"\"Backward pass for layer normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, D)\n",
    "    - cache: Variable of intermediates from layernorm_forward.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "    \"\"\"\n",
    "    \n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "    dx, dgamma, dbeta = batchnorm_backward_alt(dout.T, cache) # same as batchnorm backprop\n",
    "    dx = dx.T                                                 # transpose back dx\n",
    "\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def dropout_forward(x, dropout_param):\n",
    "    \"\"\"Forward pass for inverted dropout.\n",
    "\n",
    "    different from the vanilla version of dropout.\n",
    "    Here, p is the probability of keeping a neuron output, as opposed to the probability of dropping a neuron output.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of any shape\n",
    "    - dropout_param: A dictionary with the following keys:\n",
    "      - p: Dropout parameter. We keep each neuron output with probability p.\n",
    "      - mode: 'test' or 'train'. If the mode is train, then perform dropout;\n",
    "        if the mode is test, then just return the input.\n",
    "      - seed: Seed for the random number generator. Passing seed makes this\n",
    "        function deterministic, which is needed for gradient checking but not\n",
    "        in real networks.\n",
    "\n",
    "    Outputs:\n",
    "    - out: Array of the same shape as x.\n",
    "    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout\n",
    "      mask that was used to multiply the input; in test mode, mask is None.\n",
    "    \"\"\"\n",
    "    p, mode = dropout_param[\"p\"], dropout_param[\"mode\"]\n",
    "    if \"seed\" in dropout_param:\n",
    "        np.random.seed(dropout_param[\"seed\"])\n",
    "\n",
    "    mask = None\n",
    "    out = None\n",
    "\n",
    "    if mode == \"train\":\n",
    "        mask = np.random.choice([0, 1], size=x.shape, p=[1-p, p])/p\n",
    "        out = x*mask\n",
    "\n",
    "    elif mode == \"test\":\n",
    "        out = x\n",
    "    \n",
    "    cache = (dropout_param, mask)\n",
    "    out = out.astype(x.dtype, copy=False)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    \"\"\"Backward pass for inverted dropout.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: (dropout_param, mask) from dropout_forward.\n",
    "    \"\"\"\n",
    "    dropout_param, mask = cache\n",
    "    mode = dropout_param[\"mode\"]\n",
    "\n",
    "    dx = None\n",
    "    if mode == \"train\":\n",
    "        dx = dout*mask\n",
    "\n",
    "    elif mode == \"test\":\n",
    "        dx = dout\n",
    "    return dx\n",
    "\n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    \"\"\"A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "    The input consists of N data points, each with C channels, height H and\n",
    "    width W. We convolve each input with F different filters, where each filter\n",
    "    spans all C channels and has height HH and width WW.\n",
    "\n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - w: Filter weights of shape (F, C, HH, WW)\n",
    "    - b: Biases, of shape (F,)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n",
    "    along the height and width axes of the input. Be careful not to modfiy the original\n",
    "    input x directly.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    - cache: (x, w, b, conv_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "\n",
    "    N, C, H, W = x.shape\n",
    "    F, C, HH, WW = w.shape\n",
    "    S = conv_param['stride']\n",
    "    P = conv_param['pad']\n",
    "\n",
    "    # Add padding to each image\n",
    "    x_pad = np.pad(x, ((0,), (0,), (P,), (P,)), 'constant')\n",
    "    # Size of the output\n",
    "    Hh = 1 + (H + 2 * P - HH) // S\n",
    "    Hw = 1 + (W + 2 * P - WW) // S\n",
    "\n",
    "    out = np.zeros((N, F, Hh, Hw))\n",
    "\n",
    "    for n in range(N):  # First, iterate over all the images\n",
    "        for f in range(F):  # Second, iterate over all the kernels\n",
    "            for k in range(Hh):\n",
    "                for l in range(Hw):\n",
    "                    out[n, f, k, l] = np.sum(x_pad[n, :, (k * S):(k * S + HH), (l * S):(l * S + WW)] * w[f, :]) + b[f]\n",
    "\n",
    "    cache = (x, w, b, conv_param)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "    \"\"\"A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives.\n",
    "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x\n",
    "    - dw: Gradient with respect to w\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    dx, dw, db = None, None, None\n",
    "\n",
    "    x, w, b, conv_param = cache\n",
    "    P = conv_param['pad']\n",
    "    x_pad = np.pad(x, ((0,), (0,), (P,), (P,)), 'constant')\n",
    "\n",
    "    N, C, H, W = x.shape\n",
    "    F, C, HH, WW = w.shape\n",
    "    N, F, Hh, Hw = dout.shape\n",
    "    S = conv_param['stride']\n",
    "\n",
    "    # For dw: Size (F, C,HH,WW)\n",
    "    dw = np.zeros((F, C, HH, WW))\n",
    "    for fprime in range(F):\n",
    "        for cprime in range(C):\n",
    "            for i in range(HH):\n",
    "                for j in range(WW):\n",
    "                    sub_xpad = x_pad[:, cprime, i:(i + Hh * S):S, j:(j + Hw * S):S]\n",
    "                    dw[fprime, cprime, i, j] = np.sum(dout[:, fprime, :, :] * sub_xpad)\n",
    "\n",
    "    # For db : Size (F,)\n",
    "    db = np.zeros((F))\n",
    "    for fprime in range(F):\n",
    "        db[fprime] = np.sum(dout[:, fprime, :, :])\n",
    "\n",
    "    dx = np.zeros((N, C, H, W))\n",
    "    for nprime in range(N):\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                for f in range(F):\n",
    "                    for k in range(Hh):\n",
    "                        for l in range(Hw):\n",
    "                            mask1 = np.zeros_like(w[f, :, :, :])\n",
    "                            mask2 = np.zeros_like(w[f, :, :, :])\n",
    "                            if (i + P - k * S) < HH and (i + P - k * S) >= 0:\n",
    "                                mask1[:, i + P - k * S, :] = 1.0\n",
    "                            if (j + P - l * S) < WW and (j + P - l * S) >= 0:\n",
    "                                mask2[:, :, j + P - l * S] = 1.0\n",
    "                            w_masked = np.sum( w[f, :, :, :] * mask1 * mask2, axis=(1, 2))\n",
    "                            dx[nprime, :, i, j] += dout[nprime, f, k, l] * w_masked\n",
    "\n",
    "    return dx, dw, db\n",
    "\n",
    "\n",
    "def max_pool_forward_naive(x, pool_param):\n",
    "    \"\"\"A naive implementation of the forward pass for a max-pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C, H, W)\n",
    "    - pool_param: dictionary with the following keys:\n",
    "      - 'pool_height': The height of each pooling region\n",
    "      - 'pool_width': The width of each pooling region\n",
    "      - 'stride': The distance between adjacent pooling regions\n",
    "\n",
    "    No padding is necessary here, eg you can assume:\n",
    "      - (H - pool_height) % stride == 0\n",
    "      - (W - pool_width) % stride == 0\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, C, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H - pool_height) / stride\n",
    "      W' = 1 + (W - pool_width) / stride\n",
    "    - cache: (x, pool_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "\n",
    "    Hp = pool_param['pool_height']\n",
    "    Wp = pool_param['pool_width']\n",
    "    S = pool_param['stride']\n",
    "    N, C, H, W = x.shape\n",
    "    H1 = (H - Hp) // S + 1\n",
    "    W1 = (W - Wp) // S + 1\n",
    "\n",
    "    out = np.zeros((N, C, H1, W1))\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for k in range(H1):\n",
    "                for l in range(W1):\n",
    "                    out[n, c, k, l] = np.max(\n",
    "                        x[n, c, (k * S):(k * S + Hp), (l * S):(l * S + Wp)])\n",
    "                    \n",
    "    cache = (x, pool_param)\n",
    "    return out, cache\n",
    "\n",
    "def max_pool_backward_naive(dout, cache):\n",
    "    \"\"\"A naive implementation of the backward pass for a max-pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives\n",
    "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx = None\n",
    "\n",
    "    x, pool_param = cache\n",
    "    Hp = pool_param['pool_height']\n",
    "    Wp = pool_param['pool_width']\n",
    "    S = pool_param['stride']\n",
    "    N, C, H, W = x.shape\n",
    "    H1 = (H - Hp) // S + 1\n",
    "    W1 = (W - Wp) // S + 1\n",
    "\n",
    "    dx = np.zeros((N, C, H, W))\n",
    "    for nprime in range(N):\n",
    "        for cprime in range(C):\n",
    "            for k in range(H1):\n",
    "                for l in range(W1):\n",
    "                    x_pooling = x[nprime, cprime, (k*S):(k*S + Hp), (l*S):(l*S + Wp)]\n",
    "                    maxi = np.max(x_pooling)\n",
    "                    x_mask = x_pooling == maxi\n",
    "                    dx[nprime, cprime, (k*S):(k*S + Hp), (l*S):(l*S + Wp)] += dout[nprime, cprime, k, l] * x_mask\n",
    "\n",
    "    return dx\n",
    "\n",
    "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"Computes the forward pass for spatial batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - gamma: Scale parameter, of shape (C,)\n",
    "    - beta: Shift parameter, of shape (C,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance. momentum=0 means that\n",
    "        old information is discarded completely at every time step, while\n",
    "        momentum=1 means that new information is never incorporated. The\n",
    "        default of momentum=0.9 should work well in most situations.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, C, H, W)\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "\n",
    "    N, C, H, W = x.shape                                     # input dims\n",
    "    x = np.moveaxis(x, 1, -1).reshape(-1, C)                 # swap axes to use vanilla batchnorm\n",
    "    out, cache = batchnorm_forward(x, gamma, beta, bn_param) # perform vanilla batchnorm\n",
    "    out = np.moveaxis(out.reshape(N, H, W, C), -1, 1)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def spatial_batchnorm_backward(dout, cache):\n",
    "    \"\"\"Computes the backward pass for spatial batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, C, H, W)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
    "    - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
    "    - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "    N, C, H, W = dout.shape                             # upstream dims\n",
    "    dout = np.moveaxis(dout, 1, -1).reshape(-1, C)      # swap axes to use vanilla batchnorm backprop\n",
    "    dx, dgamma, dbeta = batchnorm_backward(dout, cache) # perform vanilla batchnorm backprop\n",
    "    dx = np.moveaxis(dx.reshape(N, H, W, C), -1, 1)     # swap back axes for the gradient of dx\n",
    "\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def spatial_groupnorm_forward(x, gamma, beta, G, gn_param):\n",
    "    \"\"\"Computes the forward pass for spatial group normalization.\n",
    "    \n",
    "    In contrast to layer normalization, group normalization splits each entry in the data into G\n",
    "    contiguous pieces, which it then normalizes independently. Per-feature shifting and scaling\n",
    "    are then applied to the data, in a manner identical to that of batch normalization and layer\n",
    "    normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - gamma: Scale parameter, of shape (1, C, 1, 1)\n",
    "    - beta: Shift parameter, of shape (1, C, 1, 1)\n",
    "    - G: Integer mumber of groups to split into, should be a divisor of C\n",
    "    - gn_param: Dictionary with the following keys:\n",
    "      - eps: Constant for numeric stability\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, C, H, W)\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "    eps = gn_param.get(\"eps\", 1e-5)\n",
    "\n",
    "    N, C, H, W = x.shape                                            # input dims\n",
    "    ln_param = {\"shape\":(W, H, C, N), \"axis\":(0, 1, 3), **gn_param} # params to reuse batchnorm method\n",
    "    \n",
    "    x = x.reshape(N*G, -1) # reshape x to use vanilla layernorm\n",
    "    gamma = np.tile(gamma, (N, 1, H, W)).reshape(N*G, -1)     # reshape gamma to use vanilla layernorm\n",
    "    beta = np.tile(beta, (N, 1, H, W)).reshape(N*G, -1)       # reshape beta to use vanilla layernorm\n",
    "\n",
    "    out, cache = layernorm_forward(x, gamma, beta, ln_param)  # perform vanilla layernorm\n",
    "    out = out.reshape(N, C, H, W)                             # reshape back the output\n",
    "    cache = (G, cache)                                      # cache involves G\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def spatial_groupnorm_backward(dout, cache):\n",
    "    \"\"\"Computes the backward pass for spatial group normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, C, H, W)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
    "    - dgamma: Gradient with respect to scale parameter, of shape (1, C, 1, 1)\n",
    "    - dbeta: Gradient with respect to shift parameter, of shape (1, C, 1, 1)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "    G, cache = cache                                    # expand cache\n",
    "    N, C, H, W = dout.shape                             # upstream dims\n",
    "    dout = dout.reshape(N*G, -1)                        # reshape to use vanilla layernorm backprop\n",
    "\n",
    "    dx, dgamma, dbeta = layernorm_backward(dout, cache) # perform vanilla layernorm backprop\n",
    "    dx = dx.reshape(N, C, H, W)                         # reshape back dx\n",
    "    dbeta = dbeta[None, :, None, None]                  # reshape back dbeta\n",
    "    dgamma = dgamma[None, :, None, None]                # reshape back dgamma\n",
    "\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def generic_forward(x, w, b, gamma=None, beta=None, bn_param=None, dropout_param=None, last=False):\n",
    "    \"\"\"Convenience layer that performs an affine transform, a batch/layer normalization if needed, a ReLU, and dropout if needed.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: Input to the affine layer\n",
    "    - w, b: Weights for the affine layer\n",
    "    - gamma, beta: Scale and shift params for the batch normalization\n",
    "    - bn_param: Dictionary of required BN parameters\n",
    "    - dropout_param: Dictionary of required Dropout parameters\n",
    "    - last: Indicates wether to perform just affine forward\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU or Dropout\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    bn_cache, ln_cache, relu_cache, dropout_cache = None, None, None, None # Initialize optional caches to None\n",
    "    out, fc_cache = affine_forward(x, w, b) # Affine forward is a must\n",
    "\n",
    "    # If the the layer is not last\n",
    "    if not last:\n",
    "        # If it has normalization layer we normalize outputs: if it bn_param has mode (train | test), it's batchnorm, otherwise, it's layernorm\n",
    "        if bn_param is not None:\n",
    "            if 'mode' in bn_param:\n",
    "                out, bn_cache = batchnorm_forward(out, gamma, beta, bn_param)\n",
    "            else:\n",
    "                out, ln_cache = layernorm_forward(out, gamma, beta, bn_param)\n",
    "\n",
    "        # Pass the outputs through non-linearity\n",
    "        out, relu_cache = relu_forward(out) # perform relu\n",
    "\n",
    "        # Use dropout if we are given its parameters\n",
    "        if dropout_param is not None:\n",
    "            out, dropout_cache = dropout_forward(out, dropout_param)\n",
    "    \n",
    "    # Prepare cache for backward pass\n",
    "    cache = fc_cache, bn_cache, ln_cache, relu_cache, dropout_cache\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def generic_backward(dout, cache):\n",
    "    \"\"\"Backward pass for the affine-bn/ln?-relu-dropout? convenience layer.\n",
    "    \"\"\"\n",
    "    # Init norm params to None\n",
    "    dgamma, dbeta = None, None\n",
    "\n",
    "    # Get the prapared caches from the forward pass\n",
    "    fc_cache, bn_cache, ln_cache, relu_cache, dropout_cache = cache\n",
    "\n",
    "    # If dropout was performed\n",
    "    if dropout_cache is not None:\n",
    "        dout = dropout_backward(dout, dropout_cache)\n",
    "    \n",
    "    # If relu was performed\n",
    "    if relu_cache is not None:\n",
    "        dout = relu_backward(dout, relu_cache)\n",
    "\n",
    "    # If norm was performed\n",
    "    if bn_cache is not None:\n",
    "        dout, dgamma, dbeta = batchnorm_backward(dout, bn_cache)\n",
    "    elif ln_cache is not None:\n",
    "        dout, dgamma, dbeta = layernorm_backward(dout, ln_cache)\n",
    "    \n",
    "    # Affine backward is a must\n",
    "    dx, dw, db = affine_backward(dout, fc_cache)\n",
    "\n",
    "    return dx, dw, db, dgamma, dbeta\n",
    "\n",
    "def affine_relu_forward(x, w, b):\n",
    "    \"\"\"Convenience layer that performs an affine transform followed by a ReLU.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the affine layer\n",
    "    - w, b: Weights for the affine layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, fc_cache = affine_forward(x, w, b)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (fc_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "def affine_relu_backward(dout, cache):\n",
    "    \"\"\"Backward pass for the affine-relu convenience layer.\n",
    "    \"\"\"\n",
    "    fc_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = affine_backward(da, fc_cache)\n",
    "    return dx, dw, db\n",
    "\n",
    "def conv_relu_forward(x, w, b, conv_param):\n",
    "    \"\"\"A convenience layer that performs a convolution followed by a ReLU.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the convolutional layer\n",
    "    - w, b, conv_param: Weights and parameters for the convolutional layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, conv_cache = conv_forward_fast(x, w, b, conv_param)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (conv_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def conv_relu_backward(dout, cache):\n",
    "    \"\"\"Backward pass for the conv-relu convenience layer.\n",
    "    \"\"\"\n",
    "    conv_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = conv_backward_fast(da, conv_cache)\n",
    "    return dx, dw, db\n",
    "\n",
    "\n",
    "def conv_bn_relu_forward(x, w, b, gamma, beta, conv_param, bn_param):\n",
    "    \"\"\"Convenience layer that performs a convolution, a batch normalization, and a ReLU.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the convolutional layer\n",
    "    - w, b, conv_param: Weights and parameters for the convolutional layer\n",
    "    - pool_param: Parameters for the pooling layer\n",
    "    - gamma, beta: Arrays of shape (D2,) and (D2,) giving scale and shift\n",
    "      parameters for batch normalization.\n",
    "    - bn_param: Dictionary of parameters for batch normalization.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the pooling layer\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, conv_cache = conv_forward_fast(x, w, b, conv_param)\n",
    "    an, bn_cache = spatial_batchnorm_forward(a, gamma, beta, bn_param)\n",
    "    out, relu_cache = relu_forward(an)\n",
    "    cache = (conv_cache, bn_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def conv_bn_relu_backward(dout, cache):\n",
    "    \"\"\"Backward pass for the conv-bn-relu convenience layer.\n",
    "    \"\"\"\n",
    "    conv_cache, bn_cache, relu_cache = cache\n",
    "    dan = relu_backward(dout, relu_cache)\n",
    "    da, dgamma, dbeta = spatial_batchnorm_backward(dan, bn_cache)\n",
    "    dx, dw, db = conv_backward_fast(da, conv_cache)\n",
    "    return dx, dw, db, dgamma, dbeta\n",
    "\n",
    "\n",
    "def conv_relu_pool_forward(x, w, b, conv_param, pool_param):\n",
    "    \"\"\"Convenience layer that performs a convolution, a ReLU, and a pool.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the convolutional layer\n",
    "    - w, b, conv_param: Weights and parameters for the convolutional layer\n",
    "    - pool_param: Parameters for the pooling layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the pooling layer\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, conv_cache = conv_forward_fast(x, w, b, conv_param)\n",
    "    s, relu_cache = relu_forward(a)\n",
    "    out, pool_cache = max_pool_forward_fast(s, pool_param)\n",
    "    cache = (conv_cache, relu_cache, pool_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def conv_relu_pool_backward(dout, cache):\n",
    "    \"\"\"Backward pass for the conv-relu-pool convenience layer.\n",
    "    \"\"\"\n",
    "    conv_cache, relu_cache, pool_cache = cache\n",
    "    ds = max_pool_backward_fast(dout, pool_cache)\n",
    "    da = relu_backward(ds, relu_cache)\n",
    "    dx, dw, db = conv_backward_fast(da, conv_cache)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast layers for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - field_height) % stride == 0\n",
    "    assert (W + 2 * padding - field_height) % stride == 0\n",
    "    out_height = (H + 2 * padding - field_height) / stride + 1\n",
    "    out_width = (W + 2 * padding - field_width) / stride + 1\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "    return (k, i, j)\n",
    "\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode=\"constant\")\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "\n",
    "def conv_forward_im2col(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A fast implementation of the forward pass for a convolutional layer\n",
    "    based on im2col and col2im.\n",
    "    \"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    num_filters, _, filter_height, filter_width = w.shape\n",
    "    stride, pad = conv_param[\"stride\"], conv_param[\"pad\"]\n",
    "\n",
    "    # Check dimensions\n",
    "    assert (W + 2 * pad - filter_width) % stride == 0, \"width does not work\"\n",
    "    assert (H + 2 * pad - filter_height) % stride == 0, \"height does not work\"\n",
    "\n",
    "    # Create output\n",
    "    out_height = (H + 2 * pad - filter_height) // stride + 1\n",
    "    out_width = (W + 2 * pad - filter_width) // stride + 1\n",
    "    out = np.zeros((N, num_filters, out_height, out_width), dtype=x.dtype)\n",
    "\n",
    "    # x_cols = im2col_indices(x, w.shape[2], w.shape[3], pad, stride)\n",
    "    x_cols = im2col_cython(x, w.shape[2], w.shape[3], pad, stride)\n",
    "    res = w.reshape((w.shape[0], -1)).dot(x_cols) + b.reshape(-1, 1)\n",
    "\n",
    "    out = res.reshape(w.shape[0], out.shape[2], out.shape[3], x.shape[0])\n",
    "    out = out.transpose(3, 0, 1, 2)\n",
    "\n",
    "    cache = (x, w, b, conv_param, x_cols)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def conv_forward_strides(x, w, b, conv_param):\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = w.shape\n",
    "    stride, pad = conv_param[\"stride\"], conv_param[\"pad\"]\n",
    "\n",
    "    # Check dimensions\n",
    "    # assert (W + 2 * pad - WW) % stride == 0, 'width does not work'\n",
    "    # assert (H + 2 * pad - HH) % stride == 0, 'height does not work'\n",
    "\n",
    "    # Pad the input\n",
    "    p = pad\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode=\"constant\")\n",
    "\n",
    "    # Figure out output dimensions\n",
    "    H += 2 * pad\n",
    "    W += 2 * pad\n",
    "    out_h = (H - HH) // stride + 1\n",
    "    out_w = (W - WW) // stride + 1\n",
    "\n",
    "    # Perform an im2col operation by picking clever strides\n",
    "    shape = (C, HH, WW, N, out_h, out_w)\n",
    "    strides = (H * W, W, 1, C * H * W, stride * W, stride)\n",
    "    strides = x.itemsize * np.array(strides)\n",
    "    x_stride = np.lib.stride_tricks.as_strided(x_padded, shape=shape, strides=strides)\n",
    "    x_cols = np.ascontiguousarray(x_stride)\n",
    "    x_cols.shape = (C * HH * WW, N * out_h * out_w)\n",
    "\n",
    "    # Now all our convolutions are a big matrix multiply\n",
    "    res = w.reshape(F, -1).dot(x_cols) + b.reshape(-1, 1)\n",
    "\n",
    "    # Reshape the output\n",
    "    res.shape = (F, N, out_h, out_w)\n",
    "    out = res.transpose(1, 0, 2, 3)\n",
    "\n",
    "    # Be nice and return a contiguous array\n",
    "    # The old version of conv_forward_fast doesn't do this, so for a fair\n",
    "    # comparison we won't either\n",
    "    out = np.ascontiguousarray(out)\n",
    "\n",
    "    cache = (x, w, b, conv_param, x_cols)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def conv_backward_strides(dout, cache):\n",
    "    x, w, b, conv_param, x_cols = cache\n",
    "    stride, pad = conv_param[\"stride\"], conv_param[\"pad\"]\n",
    "\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = w.shape\n",
    "    _, _, out_h, out_w = dout.shape\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "\n",
    "    dout_reshaped = dout.transpose(1, 0, 2, 3).reshape(F, -1)\n",
    "    dw = dout_reshaped.dot(x_cols.T).reshape(w.shape)\n",
    "\n",
    "    dx_cols = w.reshape(F, -1).T.dot(dout_reshaped)\n",
    "    dx_cols.shape = (C, HH, WW, N, out_h, out_w)\n",
    "    dx = col2im_6d_cython(dx_cols, N, C, H, W, HH, WW, pad, stride)\n",
    "\n",
    "    return dx, dw, db\n",
    "\n",
    "\n",
    "def conv_backward_im2col(dout, cache):\n",
    "    \"\"\"\n",
    "    A fast implementation of the backward pass for a convolutional layer\n",
    "    based on im2col and col2im.\n",
    "    \"\"\"\n",
    "    x, w, b, conv_param, x_cols = cache\n",
    "    stride, pad = conv_param[\"stride\"], conv_param[\"pad\"]\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "\n",
    "    num_filters, _, filter_height, filter_width = w.shape\n",
    "    dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(num_filters, -1)\n",
    "    dw = dout_reshaped.dot(x_cols.T).reshape(w.shape)\n",
    "\n",
    "    dx_cols = w.reshape(num_filters, -1).T.dot(dout_reshaped)\n",
    "    # dx = col2im_indices(dx_cols, x.shape, filter_height, filter_width, pad, stride)\n",
    "    dx = col2im_cython(\n",
    "        dx_cols,\n",
    "        x.shape[0],\n",
    "        x.shape[1],\n",
    "        x.shape[2],\n",
    "        x.shape[3],\n",
    "        filter_height,\n",
    "        filter_width,\n",
    "        pad,\n",
    "        stride,\n",
    "    )\n",
    "\n",
    "    return dx, dw, db\n",
    "\n",
    "\n",
    "conv_forward_fast = conv_forward_strides\n",
    "conv_backward_fast = conv_backward_strides\n",
    "\n",
    "\n",
    "def max_pool_forward_fast(x, pool_param):\n",
    "    \"\"\"\n",
    "    A fast implementation of the forward pass for a max pooling layer.\n",
    "\n",
    "    This chooses between the reshape method and the im2col method. If the pooling\n",
    "    regions are square and tile the input image, then we can use the reshape\n",
    "    method which is very fast. Otherwise we fall back on the im2col method, which\n",
    "    is not much faster than the naive method.\n",
    "    \"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    pool_height, pool_width = pool_param[\"pool_height\"], pool_param[\"pool_width\"]\n",
    "    stride = pool_param[\"stride\"]\n",
    "\n",
    "    same_size = pool_height == pool_width == stride\n",
    "    tiles = H % pool_height == 0 and W % pool_width == 0\n",
    "    if same_size and tiles:\n",
    "        out, reshape_cache = max_pool_forward_reshape(x, pool_param)\n",
    "        cache = (\"reshape\", reshape_cache)\n",
    "    else:\n",
    "        out, im2col_cache = max_pool_forward_im2col(x, pool_param)\n",
    "        cache = (\"im2col\", im2col_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def max_pool_backward_fast(dout, cache):\n",
    "    \"\"\"\n",
    "    A fast implementation of the backward pass for a max pooling layer.\n",
    "\n",
    "    This switches between the reshape method an the im2col method depending on\n",
    "    which method was used to generate the cache.\n",
    "    \"\"\"\n",
    "    method, real_cache = cache\n",
    "    if method == \"reshape\":\n",
    "        return max_pool_backward_reshape(dout, real_cache)\n",
    "    elif method == \"im2col\":\n",
    "        return max_pool_backward_im2col(dout, real_cache)\n",
    "    else:\n",
    "        raise ValueError('Unrecognized method \"%s\"' % method)\n",
    "\n",
    "\n",
    "def max_pool_forward_reshape(x, pool_param):\n",
    "    \"\"\"\n",
    "    A fast implementation of the forward pass for the max pooling layer that uses\n",
    "    some clever reshaping.\n",
    "\n",
    "    This can only be used for square pooling regions that tile the input.\n",
    "    \"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    pool_height, pool_width = pool_param[\"pool_height\"], pool_param[\"pool_width\"]\n",
    "    stride = pool_param[\"stride\"]\n",
    "    assert pool_height == pool_width == stride, \"Invalid pool params\"\n",
    "    assert H % pool_height == 0\n",
    "    assert W % pool_height == 0\n",
    "    x_reshaped = x.reshape(\n",
    "        N, C, H // pool_height, pool_height, W // pool_width, pool_width\n",
    "    )\n",
    "    out = x_reshaped.max(axis=3).max(axis=4)\n",
    "\n",
    "    cache = (x, x_reshaped, out)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def max_pool_backward_reshape(dout, cache):\n",
    "    \"\"\"\n",
    "    A fast implementation of the backward pass for the max pooling layer that\n",
    "    uses some clever broadcasting and reshaping.\n",
    "\n",
    "    This can only be used if the forward pass was computed using\n",
    "    max_pool_forward_reshape.\n",
    "\n",
    "    NOTE: If there are multiple argmaxes, this method will assign gradient to\n",
    "    ALL argmax elements of the input rather than picking one. In this case the\n",
    "    gradient will actually be incorrect. However this is unlikely to occur in\n",
    "    practice, so it shouldn't matter much. One possible solution is to split the\n",
    "    upstream gradient equally among all argmax elements; this should result in a\n",
    "    valid subgradient. You can make this happen by uncommenting the line below;\n",
    "    however this results in a significant performance penalty (about 40% slower)\n",
    "    and is unlikely to matter in practice so we don't do it.\n",
    "    \"\"\"\n",
    "    x, x_reshaped, out = cache\n",
    "\n",
    "    dx_reshaped = np.zeros_like(x_reshaped)\n",
    "    out_newaxis = out[:, :, :, np.newaxis, :, np.newaxis]\n",
    "    mask = x_reshaped == out_newaxis\n",
    "    dout_newaxis = dout[:, :, :, np.newaxis, :, np.newaxis]\n",
    "    dout_broadcast, _ = np.broadcast_arrays(dout_newaxis, dx_reshaped)\n",
    "    dx_reshaped[mask] = dout_broadcast[mask]\n",
    "    dx_reshaped /= np.sum(mask, axis=(3, 5), keepdims=True)\n",
    "    dx = dx_reshaped.reshape(x.shape)\n",
    "\n",
    "    return dx\n",
    "\n",
    "\n",
    "def max_pool_forward_im2col(x, pool_param):\n",
    "    \"\"\"\n",
    "    An implementation of the forward pass for max pooling based on im2col.\n",
    "\n",
    "    This isn't much faster than the naive version, so it should be avoided if\n",
    "    possible.\n",
    "    \"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    pool_height, pool_width = pool_param[\"pool_height\"], pool_param[\"pool_width\"]\n",
    "    stride = pool_param[\"stride\"]\n",
    "\n",
    "    assert (H - pool_height) % stride == 0, \"Invalid height\"\n",
    "    assert (W - pool_width) % stride == 0, \"Invalid width\"\n",
    "\n",
    "    out_height = (H - pool_height) // stride + 1\n",
    "    out_width = (W - pool_width) // stride + 1\n",
    "\n",
    "    x_split = x.reshape(N * C, 1, H, W)\n",
    "    x_cols = im2col(x_split, pool_height, pool_width, padding=0, stride=stride)\n",
    "    x_cols_argmax = np.argmax(x_cols, axis=0)\n",
    "    x_cols_max = x_cols[x_cols_argmax, np.arange(x_cols.shape[1])]\n",
    "    out = x_cols_max.reshape(out_height, out_width, N, C).transpose(2, 3, 0, 1)\n",
    "\n",
    "    cache = (x, x_cols, x_cols_argmax, pool_param)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def max_pool_backward_im2col(dout, cache):\n",
    "    \"\"\"\n",
    "    An implementation of the backward pass for max pooling based on im2col.\n",
    "\n",
    "    This isn't much faster than the naive version, so it should be avoided if\n",
    "    possible.\n",
    "    \"\"\"\n",
    "    x, x_cols, x_cols_argmax, pool_param = cache\n",
    "    N, C, H, W = x.shape\n",
    "    pool_height, pool_width = pool_param[\"pool_height\"], pool_param[\"pool_width\"]\n",
    "    stride = pool_param[\"stride\"]\n",
    "\n",
    "    dout_reshaped = dout.transpose(2, 3, 0, 1).flatten()\n",
    "    dx_cols = np.zeros_like(x_cols)\n",
    "    dx_cols[x_cols_argmax, np.arange(dx_cols.shape[1])] = dout_reshaped\n",
    "    dx = col2im_indices(\n",
    "        dx_cols, (N * C, 1, H, W), pool_height, pool_width, padding=0, stride=stride\n",
    "    )\n",
    "    dx = dx.reshape(x.shape)\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objects for Defining CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(object):\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the following architecture:\n",
    "\n",
    "    conv - relu - 2x2 max pool - affine - relu - affine - softmax\n",
    "\n",
    "    The network operates on minibatches of data that have shape (N, C, H, W)\n",
    "    consisting of N images, each with height H and width W and with C input\n",
    "    channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=(3, 32, 32),\n",
    "        num_filters=32,\n",
    "        filter_size=7,\n",
    "        hidden_dim=100,\n",
    "        num_classes=10,\n",
    "        weight_scale=1e-3,\n",
    "        reg=0.0,\n",
    "        dtype=np.float32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: Tuple (C, H, W) giving size of input data\n",
    "        - num_filters: Number of filters to use in the convolutional layer\n",
    "        - filter_size: Width/height of filters to use in the convolutional layer\n",
    "        - hidden_dim: Number of units to use in the fully-connected hidden layer\n",
    "        - num_classes: Number of scores to produce from the final affine layer.\n",
    "        - weight_scale: Scalar giving standard deviation for random initialization\n",
    "          of weights.\n",
    "        - reg: Scalar giving L2 regularization strength\n",
    "        - dtype: numpy datatype to use for computation.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        self.dtype = dtype\n",
    "\n",
    "        F, (C, H, W) = num_filters, input_dim\n",
    "\n",
    "        self.params.update({\n",
    "            'W1': np.random.randn(F, C, filter_size, filter_size) * weight_scale, \n",
    "            'b1': np.zeros(num_filters),                                          \n",
    "            'W2': np.random.randn(F * H * W // 4, hidden_dim) * weight_scale,     \n",
    "            'b2': np.zeros(hidden_dim),                                           \n",
    "            'W3': np.random.randn(hidden_dim, num_classes) * weight_scale,        \n",
    "            'b3': np.zeros(num_classes),                                          \n",
    "        })\n",
    "\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Evaluate loss and gradient for the three-layer convolutional network.\n",
    "\n",
    "        Input / output: Same API as TwoLayerNet in fc_net.py.\n",
    "        \"\"\"\n",
    "        W1, b1 = self.params[\"W1\"], self.params[\"b1\"]\n",
    "        W2, b2 = self.params[\"W2\"], self.params[\"b2\"]\n",
    "        W3, b3 = self.params[\"W3\"], self.params[\"b3\"]\n",
    "\n",
    "        # pass conv_param to the forward pass for the convolutional layer\n",
    "        # Padding and stride chosen to preserve the input spatial size\n",
    "        filter_size = W1.shape[2]\n",
    "        conv_param = {\"stride\": 1, \"pad\": (filter_size - 1) // 2}\n",
    "\n",
    "        # pass pool_param to the forward pass for the max-pooling layer\n",
    "        pool_param = {\"pool_height\": 2, \"pool_width\": 2, \"stride\": 2}\n",
    "\n",
    "        scores = None\n",
    "\n",
    "        out, cache1 = conv_relu_pool_forward(X, W1, b1, conv_param, pool_param) \n",
    "        out, cache2 = affine_relu_forward(out, W2, b2)                          \n",
    "        scores, cache3 = generic_forward(out, W3, b3, last=True)          \n",
    "\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "\n",
    "        loss, dout = softmax_loss(scores, y)                                     \n",
    "        loss += 0.5 * self.reg * (np.sum(W1**2) + np.sum(W2**2) + np.sum(W3**2)) # regularized loss\n",
    "\n",
    "        dout, grads['W3'], grads['b3'], _, _ = generic_backward(dout, cache3)    # first backward pass\n",
    "        dout, grads['W2'], grads['b2'], = affine_relu_backward(dout, cache2)     # FC backward pass\n",
    "        dout, grads['W1'], grads['b1'], = conv_relu_pool_backward(dout, cache1)  # CONV backward pass\n",
    "\n",
    "        grads['W3'] += self.reg * W3 # L2 for W3\n",
    "        grads['W2'] += self.reg * W2 # L2 for W2\n",
    "        grads['W1'] += self.reg * W1 # L2 for W1\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "class Solver(object):\n",
    "    \"\"\"\n",
    "    A Solver encapsulates all the logic necessary for training classification\n",
    "    models. The Solver performs stochastic gradient descent using different\n",
    "    update rules.\n",
    "\n",
    "    The solver accepts both training and validataion data and labels so it can\n",
    "    periodically check classification accuracy on both training and validation\n",
    "    data to watch out for overfitting.\n",
    "\n",
    "    To train a model, we first construct a Solver instance, passing the\n",
    "    model, dataset, and various options (learning rate, batch size, etc) to the\n",
    "    constructor. We then call the train() method to run the optimization\n",
    "    procedure and train the model.\n",
    "\n",
    "    After the train() method returns, model.params will contain the parameters\n",
    "    that performed best on the validation set over the course of training.\n",
    "    In addition, the instance variable solver.loss_history will contain a list\n",
    "    of all losses encountered during training and the instance variables\n",
    "    solver.train_acc_history and solver.val_acc_history will be lists of the\n",
    "    accuracies of the model on the training and validation set at each epoch.\n",
    "\n",
    "    A Solver works on a model object that must conform to the following API:\n",
    "\n",
    "    - model.params must be a dictionary mapping string parameter names to numpy\n",
    "      arrays containing parameter values.\n",
    "\n",
    "    - model.loss(X, y) must be a function that computes training-time loss and\n",
    "      gradients, and test-time classification scores, with the following inputs\n",
    "      and outputs:\n",
    "\n",
    "      Inputs:\n",
    "      - X: Array giving a minibatch of input data of shape (N, d_1, ..., d_k)\n",
    "      - y: Array of labels, of shape (N,) giving labels for X where y[i] is the\n",
    "        label for X[i].\n",
    "\n",
    "      Returns:\n",
    "      If y is None, run a test-time forward pass and return:\n",
    "      - scores: Array of shape (N, C) giving classification scores for X where\n",
    "        scores[i, c] gives the score of class c for X[i].\n",
    "\n",
    "      If y is not None, run a training time forward and backward pass and\n",
    "      return a tuple of:\n",
    "      - loss: Scalar giving the loss\n",
    "      - grads: Dictionary with the same keys as self.params mapping parameter\n",
    "        names to gradients of the loss with respect to those parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Construct a new Solver instance.\n",
    "\n",
    "        Required arguments:\n",
    "        - model: A model object conforming to the API described above\n",
    "        - data: A dictionary of training and validation data containing:\n",
    "          'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
    "          'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
    "          'y_train': Array, shape (N_train,) of labels for training images\n",
    "          'y_val': Array, shape (N_val,) of labels for validation images\n",
    "\n",
    "        Optional arguments:\n",
    "        - update_rule: A string giving the name of an update rule in optim.py.\n",
    "          Default is 'sgd'.\n",
    "        - optim_config: A dictionary containing hyperparameters that will be\n",
    "          passed to the chosen update rule. Each update rule requires different\n",
    "          hyperparameters (see optim.py) but all update rules require a\n",
    "          'learning_rate' parameter so that should always be present.\n",
    "        - lr_decay: A scalar for learning rate decay; after each epoch the\n",
    "          learning rate is multiplied by this value.\n",
    "        - batch_size: Size of minibatches used to compute loss and gradient\n",
    "          during training.\n",
    "        - num_epochs: The number of epochs to run for during training.\n",
    "        - print_every: Integer; training losses will be printed every\n",
    "          print_every iterations.\n",
    "        - verbose: Boolean; if set to false then no output will be printed\n",
    "          during training.\n",
    "        - num_train_samples: Number of training samples used to check training\n",
    "          accuracy; default is 1000; set to None to use entire training set.\n",
    "        - num_val_samples: Number of validation samples to use to check val\n",
    "          accuracy; default is None, which uses the entire validation set.\n",
    "        - checkpoint_name: If not None, then save model checkpoints here every\n",
    "          epoch.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = model\n",
    "        self.X_train = data[\"X_train\"]\n",
    "        self.y_train = data[\"y_train\"]\n",
    "        self.X_val = data[\"X_val\"]\n",
    "        self.y_val = data[\"y_val\"]\n",
    "\n",
    "        # Unpack keyword arguments\n",
    "        self.update_rule = kwargs.pop(\"update_rule\", \"sgd\")\n",
    "        self.optim_config = kwargs.pop(\"optim_config\", {})\n",
    "        self.lr_decay = kwargs.pop(\"lr_decay\", 1.0)\n",
    "        self.batch_size = kwargs.pop(\"batch_size\", 100)\n",
    "        self.num_epochs = kwargs.pop(\"num_epochs\", 10)\n",
    "        self.num_train_samples = kwargs.pop(\"num_train_samples\", 1000)\n",
    "        self.num_val_samples = kwargs.pop(\"num_val_samples\", None)\n",
    "\n",
    "        self.checkpoint_name = kwargs.pop(\"checkpoint_name\", None)\n",
    "        self.print_every = kwargs.pop(\"print_every\", 10)\n",
    "        self.verbose = kwargs.pop(\"verbose\", True)\n",
    "\n",
    "        # Throw an error if there are extra keyword arguments\n",
    "        if len(kwargs) > 0:\n",
    "            extra = \", \".join('\"%s\"' % k for k in list(kwargs.keys()))\n",
    "            raise ValueError(\"Unrecognized arguments %s\" % extra)\n",
    "        \n",
    "        self.update_rule = eval(self.update_rule)\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Set up some book-keeping variables for optimization.\n",
    "        \"\"\"\n",
    "        # Set up some variables for book-keeping\n",
    "        self.epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        self.best_params = {}\n",
    "        self.loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history = []\n",
    "\n",
    "        # Make a deep copy of the optim_config for each parameter\n",
    "        self.optim_configs = {}\n",
    "        for p in self.model.params:\n",
    "            d = {k: v for k, v in self.optim_config.items()}\n",
    "            self.optim_configs[p] = d\n",
    "\n",
    "    def _step(self):\n",
    "        \"\"\"\n",
    "        Make a single gradient update. This is called by train() and should not be called manually.\n",
    "        \"\"\"\n",
    "        # Make a minibatch of training data\n",
    "        num_train = self.X_train.shape[0]\n",
    "        batch_mask = np.random.choice(num_train, self.batch_size)\n",
    "        X_batch = self.X_train[batch_mask]\n",
    "        y_batch = self.y_train[batch_mask]\n",
    "\n",
    "        # Compute loss and gradient\n",
    "        loss, grads = self.model.loss(X_batch, y_batch)\n",
    "        self.loss_history.append(loss)\n",
    "\n",
    "        # Perform a parameter update\n",
    "        for p, w in self.model.params.items():\n",
    "            dw = grads[p]\n",
    "            config = self.optim_configs[p]\n",
    "            next_w, next_config = self.update_rule(w, dw, config)\n",
    "            self.model.params[p] = next_w\n",
    "            self.optim_configs[p] = next_config\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        if self.checkpoint_name is None:\n",
    "            return\n",
    "        checkpoint = {\n",
    "            \"model\": self.model,\n",
    "            \"update_rule\": self.update_rule,\n",
    "            \"lr_decay\": self.lr_decay,\n",
    "            \"optim_config\": self.optim_config,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"num_train_samples\": self.num_train_samples,\n",
    "            \"num_val_samples\": self.num_val_samples,\n",
    "            \"epoch\": self.epoch,\n",
    "            \"loss_history\": self.loss_history,\n",
    "            \"train_acc_history\": self.train_acc_history,\n",
    "            \"val_acc_history\": self.val_acc_history,\n",
    "        }\n",
    "        filename = \"%s_epoch_%d.pkl\" % (self.checkpoint_name, self.epoch)\n",
    "        if self.verbose:\n",
    "            print('Saving checkpoint to \"%s\"' % filename)\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(checkpoint, f)\n",
    "\n",
    "    def check_accuracy(self, X, y, num_samples=None, batch_size=100):\n",
    "        \"\"\"\n",
    "        Check accuracy of the model on the provided data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Array of data, of shape (N, d_1, ..., d_k)\n",
    "        - y: Array of labels, of shape (N,)\n",
    "        - num_samples: If not None, subsample the data and only test the model\n",
    "          on num_samples datapoints.\n",
    "        - batch_size: Split X and y into batches of this size to avoid using\n",
    "          too much memory.\n",
    "\n",
    "        Returns:\n",
    "        - acc: Scalar giving the fraction of instances that were correctly\n",
    "          classified by the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Maybe subsample the data\n",
    "        N = X.shape[0]\n",
    "        if num_samples is not None and N > num_samples:\n",
    "            mask = np.random.choice(N, num_samples)\n",
    "            N = num_samples\n",
    "            X = X[mask]\n",
    "            y = y[mask]\n",
    "\n",
    "        # Compute predictions in batches\n",
    "        num_batches = N // batch_size\n",
    "        if N % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        y_pred = []\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            scores = self.model.loss(X[start:end])\n",
    "            y_pred.append(np.argmax(scores, axis=1))\n",
    "        y_pred = np.hstack(y_pred)\n",
    "        acc = np.mean(y_pred == y)\n",
    "\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Run optimization to train the model.\n",
    "        \"\"\"\n",
    "        num_train = self.X_train.shape[0]\n",
    "        iterations_per_epoch = max(num_train // self.batch_size, 1)\n",
    "        num_iterations = self.num_epochs * iterations_per_epoch\n",
    "\n",
    "        for t in range(num_iterations):\n",
    "            self._step()\n",
    "\n",
    "            # Maybe print training loss\n",
    "            if self.verbose and t % self.print_every == 0:\n",
    "                print(\n",
    "                    \"(Iteration %d / %d) loss: %f\"\n",
    "                    % (t + 1, num_iterations, self.loss_history[-1])\n",
    "                )\n",
    "\n",
    "            # At the end of every epoch, increment the epoch counter and decay the learning rate.\n",
    "            epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "            if epoch_end:\n",
    "                self.epoch += 1\n",
    "                for k in self.optim_configs:\n",
    "                    self.optim_configs[k][\"learning_rate\"] *= self.lr_decay\n",
    "\n",
    "            # Check train and val accuracy on the first iteration, the last iteration, and at the end of each epoch.\n",
    "            first_it = t == 0\n",
    "            last_it = t == num_iterations - 1\n",
    "            if first_it or last_it or epoch_end:\n",
    "                train_acc = self.check_accuracy(\n",
    "                    self.X_train, self.y_train, num_samples=self.num_train_samples\n",
    "                )\n",
    "                val_acc = self.check_accuracy(\n",
    "                    self.X_val, self.y_val, num_samples=self.num_val_samples\n",
    "                )\n",
    "                self.train_acc_history.append(train_acc)\n",
    "                self.val_acc_history.append(val_acc)\n",
    "                self._save_checkpoint()\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\n",
    "                        \"(Epoch %d / %d) train acc: %f; val_acc: %f\"\n",
    "                        % (self.epoch, self.num_epochs, train_acc, val_acc)\n",
    "                    )\n",
    "\n",
    "                # Keep track of the best model\n",
    "                if val_acc > self.best_val_acc:\n",
    "                    self.best_val_acc = val_acc\n",
    "                    self.best_params = {}\n",
    "                    for k, v in self.model.params.items():\n",
    "                        self.best_params[k] = v.copy()\n",
    "\n",
    "        # At the end of training swap the best params into the model\n",
    "        self.model.params = self.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library of Update Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Each update rule has the same interface:\n",
    "def update(w, dw, config=None):\n",
    "\n",
    "Inputs:\n",
    "  - w: A numpy array giving the current weights.\n",
    "  - dw: A numpy array of the same shape as w giving the gradient of the\n",
    "    loss with respect to w.\n",
    "  - config: A dictionary containing hyperparameter values such as learning\n",
    "    rate, momentum, etc. If the update rule requires caching values over many\n",
    "    iterations, then config will also hold these cached values.\n",
    "\n",
    "Returns:\n",
    "  - next_w: The next point after the update.\n",
    "  - config: The config dictionary to be passed to the next iteration of the\n",
    "    update rule.\n",
    "\n",
    "For efficiency, update rules may perform in-place updates, mutating w and\n",
    "setting next_w equal to w.\n",
    "\"\"\"\n",
    "\n",
    "def sgd(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Performs vanilla stochastic gradient descent.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    config.setdefault(\"learning_rate\", 1e-2)\n",
    "\n",
    "    w -= config[\"learning_rate\"] * dw\n",
    "    return w, config\n",
    "\n",
    "\n",
    "def sgd_momentum(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent with momentum.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - momentum: Scalar between 0 and 1 giving the momentum value.\n",
    "      Setting momentum = 0 reduces to sgd.\n",
    "    - velocity: A numpy array of the same shape as w and dw used to store a\n",
    "      moving average of the gradients.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    config.setdefault(\"learning_rate\", 1e-2)\n",
    "    config.setdefault(\"momentum\", 0.9)\n",
    "    v = config.get(\"velocity\", np.zeros_like(w))\n",
    "\n",
    "    next_w = None\n",
    "    v = config[\"momentum\"]*v - config[\"learning_rate\"]*dw\n",
    "    next_w = w + v\n",
    "    config[\"velocity\"] = v\n",
    "\n",
    "    return next_w, config\n",
    "\n",
    "\n",
    "def rmsprop(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Uses the RMSProp update rule, which uses a moving average of squared\n",
    "    gradient values to set adaptive per-parameter learning rates.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n",
    "      gradient cache.\n",
    "    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "    - cache: Moving average of second moments of gradients.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    config.setdefault(\"learning_rate\", 1e-2)\n",
    "    config.setdefault(\"decay_rate\", 0.99)\n",
    "    config.setdefault(\"epsilon\", 1e-8)\n",
    "    config.setdefault(\"cache\", np.zeros_like(w))\n",
    "\n",
    "    next_w = None\n",
    "\n",
    "    cache = config[\"decay_rate\"]*config[\"cache\"] + (1-config[\"decay_rate\"])*dw**2\n",
    "    next_w = w - config[\"learning_rate\"]*dw/(np.sqrt(cache) + config[\"epsilon\"])\n",
    "    config[\"cache\"] = cache\n",
    "\n",
    "    return next_w, config\n",
    "\n",
    "def adam(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Uses the Adam update rule, which incorporates moving averages of both the\n",
    "    gradient and its square and a bias correction term.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - beta1: Decay rate for moving average of first moment of gradient.\n",
    "    - beta2: Decay rate for moving average of second moment of gradient.\n",
    "    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "    - m: Moving average of gradient.\n",
    "    - v: Moving average of squared gradient.\n",
    "    - t: Iteration number.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    config.setdefault(\"learning_rate\", 1e-3)\n",
    "    config.setdefault(\"beta1\", 0.9)\n",
    "    config.setdefault(\"beta2\", 0.999)\n",
    "    config.setdefault(\"epsilon\", 1e-8)\n",
    "    config.setdefault(\"m\", np.zeros_like(w))\n",
    "    config.setdefault(\"v\", np.zeros_like(w))\n",
    "    config.setdefault(\"t\", 0)\n",
    "\n",
    "    next_w = None\n",
    "    t = config[\"t\"] + 1 \n",
    "    m = config[\"beta1\"]*config[\"m\"] + (1-config[\"beta1\"])*dw\n",
    "    m_t = m/(1-config[\"beta1\"]**t)\n",
    "    v = config[\"beta2\"]*config[\"v\"] + (1-config[\"beta2\"])*dw**2\n",
    "    v_t = v/(1-config[\"beta2\"]**t)\n",
    "\n",
    "    next_w = w - config[\"learning_rate\"]*m_t/(np.sqrt(v_t)+config[\"epsilon\"])\n",
    "    config[\"m\"] = m\n",
    "    config[\"v\"] = v\n",
    "    config[\"t\"] = t\n",
    "\n",
    "    return next_w, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Auxiliarry Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    \"\"\"\n",
    "    a naive implementation of numerical gradient of f at x\n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \"\"\"\n",
    "\n",
    "    fx = f(x)  # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h)  # the slope\n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext()  # step to next dimension\n",
    "\n",
    "    return grad\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\"Returns relative error.\"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == \"2\":\n",
    "        return pickle.load(f)\n",
    "    elif version[0] == \"3\":\n",
    "        return pickle.load(f, encoding=\"latin1\")\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict[\"data\"]\n",
    "        Y = datadict[\"labels\"]\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, \"data_batch_%d\" % (b,))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, \"test_batch\"))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "def get_CIFAR10_data(\n",
    "    num_training=49000, num_validation=1000, num_test=1000, subtract_mean=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for classifiers. These are the same steps as we used for the SVM, but\n",
    "    condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs/datasets/cifar-10-batches-py' # path to the data on your computer\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    if subtract_mean:\n",
    "        mean_image = np.mean(X_train, axis=0)\n",
    "        X_train -= mean_image\n",
    "        X_val -= mean_image\n",
    "        X_test -= mean_image\n",
    "\n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "\n",
    "    # Package data into a dictionary\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CIFAR-10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (49000, 3, 32, 32)\n",
      "y_train: (49000,)\n",
      "X_val: (1000, 3, 32, 32)\n",
      "y_val: (1000,)\n",
      "X_test: (1000, 3, 32, 32)\n",
      "y_test: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR-10 data.\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "test for the forward pass implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_naive\n",
      "difference:  2.2121476417505994e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around e-8\n",
    "print('Testing conv_forward_naive')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "test for the backward pass implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_naive function\n",
      "dx error:  1.159803161159293e-08\n",
      "dw error:  2.2471230668641297e-10\n",
      "db error:  3.3726153958780465e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around e-8 or less.\n",
    "print('Testing conv_backward_naive function')\n",
    "print('dx error: ', rel_error(dx, dx_num))\n",
    "print('dw error: ', rel_error(dw, dw_num))\n",
    "print('db error: ', rel_error(db, db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN: Max-Pooling\n",
    "test for the max-pool forward pass implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_forward_naive function:\n",
      "difference:  4.1666665157267834e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Compare your output with ours. Difference should be on the order of e-8.\n",
    "print('Testing max_pool_forward_naive function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN: Max-Pooling Backward\n",
    "test for the max-pool backward pass implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_backward_naive function:\n",
      "dx error:  3.27562514223145e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = max_pool_forward_naive(x, pool_param)\n",
    "dx = max_pool_backward_naive(dout, cache)\n",
    "\n",
    "# Your error should be on the order of e-12\n",
    "print('Testing max_pool_backward_naive function:')\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Layers\n",
    "\n",
    "The fast convolution implementation depends on a Cython extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marian23/Documents/Projects/ML/cs231/assignment2/cs\n",
      "running build_ext\n",
      "copying build/lib.macosx-10.9-x86_64-3.9/im2col_cython.cpython-39-darwin.so -> \n"
     ]
    }
   ],
   "source": [
    "# Remember to restart the runtime after executing this cell!\n",
    "%cd cs\n",
    "!python setup.py build_ext --inplace\n",
    "#%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from im2col_cython import col2im_cython, im2col_cython\n",
    "    from im2col_cython import col2im_6d_cython\n",
    "except ImportError:\n",
    "    print(\"\\tNeed to compile a Cython extension.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing naive and fast CNN implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rel errors should be around e-9 or less.\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(100, 3, 31, 31)\n",
    "w = np.random.randn(25, 3, 3, 3)\n",
    "b = np.random.randn(25,)\n",
    "dout = np.random.randn(100, 25, 16, 16)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = conv_forward_naive(x, w, b, conv_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = conv_forward_fast(x, w, b, conv_param)\n",
    "t2 = time()\n",
    "\n",
    "print('Testing conv_forward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('Difference: ', rel_error(out_naive, out_fast))\n",
    "\n",
    "t0 = time()\n",
    "dx_naive, dw_naive, db_naive = conv_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast, dw_fast, db_fast = conv_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print('\\nTesting conv_backward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('dx difference: ', rel_error(dx_naive, dx_fast))\n",
    "print('dw difference: ', rel_error(dw_naive, dw_fast))\n",
    "print('db difference: ', rel_error(db_naive, db_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pool_forward_fast:\n",
      "Naive: 0.283614s\n",
      "fast: 0.002280s\n",
      "speedup: 124.392241x\n",
      "difference:  0.0\n",
      "\n",
      "Testing pool_backward_fast:\n",
      "Naive: 0.833200s\n",
      "fast: 0.010266s\n",
      "speedup: 81.160570x\n",
      "dx difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Relative errors should be close to 0.0.\n",
    "\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(100, 3, 32, 32)\n",
    "dout = np.random.randn(100, 3, 16, 16)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = max_pool_forward_naive(x, pool_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = max_pool_forward_fast(x, pool_param)\n",
    "t2 = time()\n",
    "\n",
    "print('Testing pool_forward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('fast: %fs' % (t2 - t1))\n",
    "print('speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('difference: ', rel_error(out_naive, out_fast))\n",
    "\n",
    "t0 = time()\n",
    "dx_naive = max_pool_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast = max_pool_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print('\\nTesting pool_backward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('fast: %fs' % (t2 - t1))\n",
    "print('speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('dx difference: ', rel_error(dx_naive, dx_fast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional \"Sandwich\" Layers\n",
    "sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_relu_pool\n",
      "dx error:  6.514336569263308e-09\n",
      "dw error:  9.321052849277948e-09\n",
      "db error:  3.57960501324485e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 16, 16)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "out, cache = conv_relu_pool_forward(x, w, b, conv_param, pool_param)\n",
    "dx, dw, db = conv_relu_pool_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], b, dout)\n",
    "\n",
    "# Relative errors should be around e-8 or less\n",
    "print('Testing conv_relu_pool')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_relu:\n",
      "dx error:  2.4140762125965004e-09\n",
      "dw error:  5.583091605859923e-10\n",
      "db error:  2.623760849972345e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 8, 8)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "out, cache = conv_relu_forward(x, w, b, conv_param)\n",
    "dx, dw, db = conv_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_forward(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_forward(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_forward(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "# Relative errors should be around e-8 or less\n",
    "print('Testing conv_relu:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-Layer Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss (no regularization):  2.302586071243987\n",
      "Initial loss (with regularization):  2.508255638232932\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet()\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (no regularization): ', loss)\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (with regularization): ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max relative error: 3.053965e-04\n",
      "W2 max relative error: 1.822723e-02\n",
      "W3 max relative error: 3.422399e-04\n",
      "b1 max relative error: 3.397321e-06\n",
      "b2 max relative error: 2.517459e-03\n",
      "b3 max relative error: 9.711800e-10\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "np.random.seed(231)\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerConvNet(\n",
    "    num_filters=3,\n",
    "    filter_size=3,\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=7,\n",
    "    dtype=np.float64\n",
    ")\n",
    "loss, grads = model.loss(X, y)\n",
    "# Errors should be small, but correct implementations may have\n",
    "# relative errors up to the order of e-2\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit Small Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 30) loss: 2.414060\n",
      "(Epoch 0 / 15) train acc: 0.200000; val_acc: 0.137000\n",
      "(Iteration 2 / 30) loss: 3.102925\n",
      "(Epoch 1 / 15) train acc: 0.140000; val_acc: 0.087000\n",
      "(Iteration 3 / 30) loss: 2.270330\n",
      "(Iteration 4 / 30) loss: 2.096705\n",
      "(Epoch 2 / 15) train acc: 0.240000; val_acc: 0.094000\n",
      "(Iteration 5 / 30) loss: 1.838880\n",
      "(Iteration 6 / 30) loss: 1.934188\n",
      "(Epoch 3 / 15) train acc: 0.510000; val_acc: 0.173000\n",
      "(Iteration 7 / 30) loss: 1.827912\n",
      "(Iteration 8 / 30) loss: 1.639574\n",
      "(Epoch 4 / 15) train acc: 0.520000; val_acc: 0.188000\n",
      "(Iteration 9 / 30) loss: 1.330082\n",
      "(Iteration 10 / 30) loss: 1.756115\n",
      "(Epoch 5 / 15) train acc: 0.630000; val_acc: 0.167000\n",
      "(Iteration 11 / 30) loss: 1.024162\n",
      "(Iteration 12 / 30) loss: 1.041826\n",
      "(Epoch 6 / 15) train acc: 0.750000; val_acc: 0.229000\n",
      "(Iteration 13 / 30) loss: 1.142777\n",
      "(Iteration 14 / 30) loss: 0.835706\n",
      "(Epoch 7 / 15) train acc: 0.790000; val_acc: 0.247000\n",
      "(Iteration 15 / 30) loss: 0.587786\n",
      "(Iteration 16 / 30) loss: 0.645509\n",
      "(Epoch 8 / 15) train acc: 0.820000; val_acc: 0.252000\n",
      "(Iteration 17 / 30) loss: 0.786844\n",
      "(Iteration 18 / 30) loss: 0.467054\n",
      "(Epoch 9 / 15) train acc: 0.820000; val_acc: 0.178000\n",
      "(Iteration 19 / 30) loss: 0.429880\n",
      "(Iteration 20 / 30) loss: 0.635498\n",
      "(Epoch 10 / 15) train acc: 0.900000; val_acc: 0.206000\n",
      "(Iteration 21 / 30) loss: 0.365807\n",
      "(Iteration 22 / 30) loss: 0.284220\n",
      "(Epoch 11 / 15) train acc: 0.820000; val_acc: 0.201000\n",
      "(Iteration 23 / 30) loss: 0.469343\n",
      "(Iteration 24 / 30) loss: 0.509369\n",
      "(Epoch 12 / 15) train acc: 0.920000; val_acc: 0.211000\n",
      "(Iteration 25 / 30) loss: 0.111638\n",
      "(Iteration 26 / 30) loss: 0.145388\n",
      "(Epoch 13 / 15) train acc: 0.930000; val_acc: 0.213000\n",
      "(Iteration 27 / 30) loss: 0.155575\n",
      "(Iteration 28 / 30) loss: 0.143398\n",
      "(Epoch 14 / 15) train acc: 0.960000; val_acc: 0.212000\n",
      "(Iteration 29 / 30) loss: 0.158160\n",
      "(Iteration 30 / 30) loss: 0.118934\n",
      "(Epoch 15 / 15) train acc: 0.990000; val_acc: 0.220000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(\n",
    "    model,\n",
    "    small_data,\n",
    "    num_epochs=15,\n",
    "    batch_size=50,\n",
    "    update_rule='adam',\n",
    "    optim_config={'learning_rate': 1e-3,},\n",
    "    verbose=True,\n",
    "    print_every=1\n",
    ")\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "test": "small_data_train_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small data training accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Print final training accuracy.\n",
    "print(\n",
    "    \"Small data training accuracy:\",\n",
    "    solver.check_accuracy(small_data['X_train'], small_data['y_train'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "test": "small_data_validation_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small data validation accuracy: 0.252\n"
     ]
    }
   ],
   "source": [
    "# Print final validation accuracy.\n",
    "print(\n",
    "    \"Small data validation accuracy:\",\n",
    "    solver.check_accuracy(small_data['X_val'], small_data['y_val'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "Training the three-layer convolutional network for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 980) loss: 2.304740\n",
      "(Epoch 0 / 1) train acc: 0.103000; val_acc: 0.107000\n",
      "(Iteration 21 / 980) loss: 2.098229\n",
      "(Iteration 41 / 980) loss: 1.949788\n",
      "(Iteration 61 / 980) loss: 1.888398\n",
      "(Iteration 81 / 980) loss: 1.877093\n",
      "(Iteration 101 / 980) loss: 1.851877\n",
      "(Iteration 121 / 980) loss: 1.859353\n",
      "(Iteration 141 / 980) loss: 1.800181\n",
      "(Iteration 161 / 980) loss: 2.143292\n",
      "(Iteration 181 / 980) loss: 1.830573\n",
      "(Iteration 201 / 980) loss: 2.037280\n",
      "(Iteration 221 / 980) loss: 2.020304\n",
      "(Iteration 241 / 980) loss: 1.823728\n",
      "(Iteration 261 / 980) loss: 1.692679\n",
      "(Iteration 281 / 980) loss: 1.882594\n",
      "(Iteration 301 / 980) loss: 1.798261\n",
      "(Iteration 321 / 980) loss: 1.851960\n",
      "(Iteration 341 / 980) loss: 1.716323\n",
      "(Iteration 361 / 980) loss: 1.897655\n",
      "(Iteration 381 / 980) loss: 1.319744\n",
      "(Iteration 401 / 980) loss: 1.738790\n",
      "(Iteration 421 / 980) loss: 1.488866\n",
      "(Iteration 441 / 980) loss: 1.718409\n",
      "(Iteration 461 / 980) loss: 1.744440\n",
      "(Iteration 481 / 980) loss: 1.605460\n",
      "(Iteration 501 / 980) loss: 1.494847\n",
      "(Iteration 521 / 980) loss: 1.835179\n",
      "(Iteration 541 / 980) loss: 1.483923\n",
      "(Iteration 561 / 980) loss: 1.676871\n",
      "(Iteration 581 / 980) loss: 1.438325\n",
      "(Iteration 601 / 980) loss: 1.443469\n",
      "(Iteration 621 / 980) loss: 1.529369\n",
      "(Iteration 641 / 980) loss: 1.763475\n",
      "(Iteration 661 / 980) loss: 1.790329\n",
      "(Iteration 681 / 980) loss: 1.693343\n",
      "(Iteration 701 / 980) loss: 1.637078\n",
      "(Iteration 721 / 980) loss: 1.644564\n",
      "(Iteration 741 / 980) loss: 1.708919\n",
      "(Iteration 761 / 980) loss: 1.494252\n",
      "(Iteration 781 / 980) loss: 1.901751\n",
      "(Iteration 801 / 980) loss: 1.898991\n",
      "(Iteration 821 / 980) loss: 1.489988\n",
      "(Iteration 841 / 980) loss: 1.377615\n",
      "(Iteration 861 / 980) loss: 1.763751\n",
      "(Iteration 881 / 980) loss: 1.540284\n",
      "(Iteration 901 / 980) loss: 1.525582\n",
      "(Iteration 921 / 980) loss: 1.674166\n",
      "(Iteration 941 / 980) loss: 1.714316\n",
      "(Iteration 961 / 980) loss: 1.534668\n",
      "(Epoch 1 / 1) train acc: 0.504000; val_acc: 0.499000\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=500, reg=0.001)\n",
    "\n",
    "solver = Solver(\n",
    "    model,\n",
    "    data,\n",
    "    num_epochs=1,\n",
    "    batch_size=50,\n",
    "    update_rule='adam',\n",
    "    optim_config={'learning_rate': 1e-3,},\n",
    "    verbose=True,\n",
    "    print_every=20\n",
    ")\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "test": "full_data_train_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data training accuracy: 0.4761836734693878\n"
     ]
    }
   ],
   "source": [
    "# Print final training accuracy.\n",
    "print(\n",
    "    \"Full data training accuracy:\",\n",
    "    solver.check_accuracy(data['X_train'], data['y_train'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "test": "full_data_validation_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data validation accuracy: 0.499\n"
     ]
    }
   ],
   "source": [
    "# Print final validation accuracy.\n",
    "print(\n",
    "    \"Full data validation accuracy:\",\n",
    "    solver.check_accuracy(data['X_val'], data['y_val'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Batch Normalization\n",
    "Spatial batch normalization computes a mean and variance for each of the `C` feature channels by computing statistics over the minibatch dimension `N` as well the spatial dimensions `H` and `W`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Batch Normalization: Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before spatial batch normalization:\n",
      "  shape:  (2, 3, 4, 5)\n",
      "  means:  [9.33463814 8.90909116 9.11056338]\n",
      "  stds:  [3.61447857 3.19347686 3.5168142 ]\n",
      "After spatial batch normalization:\n",
      "  shape:  (2, 3, 4, 5)\n",
      "  means:  [ 6.18949336e-16  5.99520433e-16 -1.22124533e-16]\n",
      "  stds:  [0.99999962 0.99999951 0.9999996 ]\n",
      "After spatial batch normalization (nontrivial gamma, beta):\n",
      "  shape:  (2, 3, 4, 5)\n",
      "  means:  [6. 7. 8.]\n",
      "  stds:  [2.99999885 3.99999804 4.99999798]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after spatial batch normalization.\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 4 * np.random.randn(N, C, H, W) + 10\n",
    "\n",
    "print('Before spatial batch normalization:')\n",
    "print('  shape: ', x.shape)\n",
    "print('  means: ', x.mean(axis=(0, 2, 3)))\n",
    "print('  stds: ', x.std(axis=(0, 2, 3)))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "gamma, beta = np.ones(C), np.zeros(C)\n",
    "bn_param = {'mode': 'train'}\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print('After spatial batch normalization:')\n",
    "print('  shape: ', out.shape)\n",
    "print('  means: ', out.mean(axis=(0, 2, 3)))\n",
    "print('  stds: ', out.std(axis=(0, 2, 3)))\n",
    "\n",
    "# Means should be close to beta and stds close to gamma\n",
    "gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print('After spatial batch normalization (nontrivial gamma, beta):')\n",
    "print('  shape: ', out.shape)\n",
    "print('  means: ', out.mean(axis=(0, 2, 3)))\n",
    "print('  stds: ', out.std(axis=(0, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After spatial batch normalization (test-time):\n",
      "  means:  [-0.08034406  0.07562881  0.05716371  0.04378383]\n",
      "  stds:  [0.96718744 1.0299714  1.02887624 1.00585577]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "N, C, H, W = 10, 4, 11, 12\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(C)\n",
    "beta = np.zeros(C)\n",
    "for t in range(50):\n",
    "  x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "  spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "a_norm, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After spatial batch normalization (test-time):')\n",
    "print('  means: ', a_norm.mean(axis=(0, 2, 3)))\n",
    "print('  stds: ', a_norm.std(axis=(0, 2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Batch Normalization: Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  2.786648193872555e-07\n",
      "dgamma error:  7.0974817113608705e-12\n",
      "dbeta error:  3.275608725278405e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 5 * np.random.randn(N, C, H, W) + 12\n",
    "gamma = np.random.randn(C)\n",
    "beta = np.random.randn(C)\n",
    "dout = np.random.randn(N, C, H, W)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "#You should expect errors of magnitudes between 1e-12~1e-06\n",
    "_, cache = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = spatial_batchnorm_backward(dout, cache)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Group Normalization\n",
    "In contrast to Layer Normalization, where you normalize over the entire feature per-datapoint, we do splitting of each per-datapoint feature into G groups and a per-group per-datapoint normalization. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/cs231n/cs231n.github.io/master/assets/a2/normalization.png\">\n",
    "</p>\n",
    "<center>Visual comparison of the normalization techniques discussed so far (image edited from paper below)</center>\n",
    "\n",
    "[Wu, Yuxin, and Kaiming He. \"Group Normalization.\" arXiv preprint arXiv:1803.08494 (2018).](https://arxiv.org/abs/1803.08494)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Group Normalization: Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before spatial group normalization:\n",
      "  shape:  (2, 6, 4, 5)\n",
      "  means:  [9.72505327 8.51114185 8.9147544  9.43448077]\n",
      "  stds:  [3.67070958 3.09892597 4.27043622 3.97521327]\n",
      "After spatial group normalization:\n",
      "  shape:  (2, 6, 4, 5)\n",
      "  means:  [-2.14643118e-16  5.25505565e-16  2.65528340e-16 -3.38618023e-16]\n",
      "  stds:  [0.99999963 0.99999948 0.99999973 0.99999968]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after spatial batch normalization.\n",
    "N, C, H, W = 2, 6, 4, 5\n",
    "G = 2\n",
    "x = 4 * np.random.randn(N, C, H, W) + 10\n",
    "x_g = x.reshape((N*G,-1))\n",
    "print('Before spatial group normalization:')\n",
    "print('  shape: ', x.shape)\n",
    "print('  means: ', x_g.mean(axis=1))\n",
    "print('  stds: ', x_g.std(axis=1))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "gamma, beta = np.ones((1,C,1,1)), np.zeros((1,C,1,1))\n",
    "bn_param = {'mode': 'train'}\n",
    "\n",
    "out, _ = spatial_groupnorm_forward(x, gamma, beta, G, bn_param)\n",
    "out_g = out.reshape((N*G,-1))\n",
    "print('After spatial group normalization:')\n",
    "print('  shape: ', out.shape)\n",
    "print('  means: ', out_g.mean(axis=1))\n",
    "print('  stds: ', out_g.std(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Group Normalization: Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  7.413109437563619e-08\n",
      "dgamma error:  9.468195772749234e-12\n",
      "dbeta error:  3.354494437653335e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, C, H, W = 2, 6, 4, 5\n",
    "G = 2\n",
    "x = 5 * np.random.randn(N, C, H, W) + 12\n",
    "gamma = np.random.randn(1,C,1,1)\n",
    "beta = np.random.randn(1,C,1,1)\n",
    "dout = np.random.randn(N, C, H, W)\n",
    "\n",
    "gn_param = {}\n",
    "fx = lambda x: spatial_groupnorm_forward(x, gamma, beta, G, gn_param)[0]\n",
    "fg = lambda a: spatial_groupnorm_forward(x, gamma, beta, G, gn_param)[0]\n",
    "fb = lambda b: spatial_groupnorm_forward(x, gamma, beta, G, gn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = spatial_groupnorm_forward(x, gamma, beta, G, gn_param)\n",
    "dx, dgamma, dbeta = spatial_groupnorm_backward(dout, cache)\n",
    "\n",
    "# You should expect errors of magnitudes between 1e-12 and 1e-07. \n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
